{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siRNA knockdown fixed ##\n",
    "This notebook fits traces with fixed mRNA expression.\n",
    "\n",
    "An amount of mRNA is added to the cells.\n",
    "At a certain time $t=0$, a fixing agent (CHX) is added, and mRNA expression is stopped.\n",
    "Now, an initial amount $G_{u0}$ of pre-mature protein and an initial amount $G_0$ of mature protein is in the cell.\n",
    "\n",
    "After adding the fixing agent, the pre-mature protein can mature with maturation rate $k_m$, and both pre-mature and mature protein can degrade with degradation rate $\\beta$.\n",
    "\n",
    "This system can be described by the following differential equations:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\mathrm{d}G_u}{\\mathrm{d}t} &= -\\beta G_u - k_m G_u \\\\\n",
    "\\frac{\\mathrm{d}G}{\\mathrm{d}t} &= -\\beta G + k_m G_u\n",
    "\\end{align*}$$\n",
    "\n",
    "The solution for the amount of mature protein $G(t)$ is:\n",
    "$$\n",
    "G(t) = G_0 \\mathrm{e}^{-\\beta t} + G_{u0}\\left(\\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t}\\right)\n",
    "$$\n",
    "The parameters to fit are $\\beta$, $k_m$ and $G_{u0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook structure\n",
    "The notebook has the following structure:\n",
    "\n",
    "At first, the model functions are defined and the data is loaded. The next section contains code for fitting the two models separately. The next section contains code for fitting the two traces in one run with parameters shared among the models.\n",
    "\n",
    "Fitting requires that the result list `R` is defined, which can be done by running the corresponding cell. When `R` has been populated by fitting, the results can be plotted. There are cells for plotting the results of the separate fit, the results of the combined fit, and the pure parameter distributions of all fits.\n",
    "\n",
    "Additionally, there are cells for saving and loading paramaters by python’s `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules needed\n",
    "\n",
    "# Standard library\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "import inspect\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Scientific stack\n",
    "import numpy as np\n",
    "np.seterr(divide='print')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import scipy as sc\n",
    "#import scipy.optimize as so\n",
    "#import scipy.stats as ss\n",
    "import sklearn.cluster as skc\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.gridspec import GridSpec\n",
    "#import matplotlib.lines as mlin\n",
    "#import matplotlib.patches as mptch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Notebook utilities\n",
    "import IPython\n",
    "import ipywidgets as wdg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def getTimeStamp():\n",
    "    \"\"\"Returns a human-readable string representation of the current time\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime(\"%Y-%m-%d–%H%M%S\")\n",
    "\n",
    "\n",
    "def getOutpath(filename='', timestamp=None):\n",
    "    \"\"\"Returns (and creates, if necessary) the path to a directory\n",
    "    called “out” inside the current directory.\n",
    "    If `filename` is given, the filename is appended to the output directory.\n",
    "    A timestamp will be added to the filename if `timestamp != ''`.\n",
    "    If timestamp is `None`, the current timestamp is used.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    outpath = os.path.join(os.getcwd(), 'out')\n",
    "    if not os.path.isdir(outpath) and not os.path.lexists(outpath):\n",
    "        os.mkdir(outpath)\n",
    "\n",
    "    # If requested, build filename\n",
    "    if len(filename) > 0:\n",
    "        if timestamp == None:\n",
    "            timestamp = getTimeStamp()\n",
    "        outpath = os.path.join(outpath, ((timestamp + '_') if len(timestamp) > 0 else '') + filename)\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t, G0, Gu0, beta, km):\n",
    "    \"\"\"General fixed expression model function\"\"\"\n",
    "    return G0 * np.exp(-beta * t) + Gu0 * (np.exp(-beta * t) - np.exp(-(beta+km) * t))\n",
    "\n",
    "def red(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Model function for fixed RFP data\"\"\"\n",
    "    return model(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Model function for fixed GFP data\"\"\"\n",
    "    return model(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default parameter values\n",
    "G0r_0 = 100\n",
    "Gu0r_0 = 1000\n",
    "betr_0 = 0.04\n",
    "kmr_0 = 0.3\n",
    "\n",
    "G0g_0 = 2000\n",
    "Gu0g_0 = 2000\n",
    "betg_0 = 0.04\n",
    "kmg_0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitParameters:\n",
    "    \"\"\"FitParameters facilitates managing values and bounds of fit parameters\"\"\"\n",
    "    def __init__(self, fun, independent=[], fixed=[]):\n",
    "        # Store function\n",
    "        self.fun = fun\n",
    "\n",
    "        # Get parameters of fun\n",
    "        params = inspect.signature(self.fun).parameters\n",
    "\n",
    "        # Build data frame of parameters\n",
    "        self.df = pd.DataFrame(columns=['value', 'min', 'max'],\n",
    "                               index=[p for p in params.keys()],\n",
    "                               dtype=np.float64)\n",
    "\n",
    "        # Set “independent” and “fixed” flag\n",
    "        self.df.add(pd.DataFrame(columns=['independent', 'fixed'], dtype=np.bool))\n",
    "        for p in self.df.index.values:\n",
    "            self.df.loc[p, 'independent'] = p in independent\n",
    "            self.df.loc[p, 'fixed'] = p in fixed\n",
    "\n",
    "        # Set default parameters\n",
    "        for p in self.df.index.values:\n",
    "            if params[p].default == inspect.Parameter.empty:\n",
    "                if self.df.loc[p, 'independent']:\n",
    "                    self.df.loc[p, 'value'] = np.NaN\n",
    "                else:\n",
    "                    self.df.loc[p, 'value'] = 0\n",
    "            else:\n",
    "                self.df.loc[p, 'value'] = params[p].default\n",
    "\n",
    "    def set(self, p, **props):\n",
    "        \"\"\"Allows user to change parameter properties\"\"\"\n",
    "        if p not in self.df.index.values:\n",
    "            raise KeyError(\"Unknown parameter name: {}\".format(par))\n",
    "\n",
    "        for prop, val in props.items():\n",
    "            if prop == 'value':\n",
    "                self.df.loc[p, 'value'] = val\n",
    "            elif prop == 'min':\n",
    "                self.df.loc[p, 'min'] = val\n",
    "            elif prop == 'max':\n",
    "                self.df.loc[p, 'max'] = val\n",
    "            elif prop == 'independent':\n",
    "                self.df.loc[p, 'independent'] = val\n",
    "            elif prop == 'fixed':\n",
    "                self.df.loc[p, 'fixed'] = val\n",
    "            else:\n",
    "                raise KeyError(\"Illegal parameter property: {}\".format(prop))\n",
    "\n",
    "    def eval_params(self, params=[], independent=True, **vals):\n",
    "        \"\"\"Returns parameters for evaluating the function.\n",
    "\n",
    "        Arguments:\n",
    "        params: optional list of values of free parameters\n",
    "        independent: optional switch whether independent variables are requested\n",
    "        vals: dictionary of parameter values\n",
    "\n",
    "        If a value for a parameter is specified in both `params` and `vals`,\n",
    "        the value from `vals` is used.\n",
    "        Values for independent parameters must be specified in `vals`.\n",
    "        If `independent == False`, the independent variable needn’t be specified\n",
    "        and will not be returned.\"\"\"\n",
    "        # Add additional values from `params` to vals\n",
    "        if len(params) != 0:\n",
    "            par_names = self.names()\n",
    "            if np.size(par_names) != len(params):\n",
    "                raise ValueError(\"Wrong number of parameters given ({})\".format(len(params)))\n",
    "            for pn, pv in zip(par_names, params):\n",
    "                if pn not in vals:\n",
    "                    vals[pn] = pv\n",
    "\n",
    "        # Fill values unspecified so far from `self.df`\n",
    "        for p in self.df.index.values:\n",
    "            if p not in vals:\n",
    "                if independent and self.df.loc[p, 'independent']:\n",
    "                    raise ValueError(\"Independent parameter `{}` not specified\".format(p))\n",
    "                else:\n",
    "                    vals[p] = self.df.loc[p, 'value']\n",
    "            elif not independent and self.df.loc[p, 'independent']:\n",
    "                del vals[p]\n",
    "        return vals\n",
    "\n",
    "    def eval(self, params=[], **vals):\n",
    "        \"\"\"Evaluates the function.\n",
    "\n",
    "        Arguments:\n",
    "        params: optional list of values of free parameters\n",
    "        vals: dictionary of parameter values\n",
    "\n",
    "        If a value for a parameter is specified in both `params` and `vals`,\n",
    "        the value from `vals` is used.\n",
    "        Values for independent parameters must be specified in `vals`.\"\"\"\n",
    "        return self.fun(**self.eval_params(params, **vals))\n",
    "\n",
    "    def freeIdx(self):\n",
    "        \"\"\"Returns a list of names of free parameters\"\"\"\n",
    "        return [p for p in self.df.index.values\n",
    "                if not (self.df.loc[p, 'independent'] or self.df.loc[p, 'fixed'])]\n",
    "\n",
    "    def bounds(self):\n",
    "        \"\"\"Returns a list of bound tuples of free parameters\n",
    "        for use in scipy.optimize.minimize\"\"\"\n",
    "        bnds = []\n",
    "        for p in self.freeIdx():\n",
    "            # Get parameter bounds\n",
    "            min_val = self.df.loc[p, 'min']\n",
    "            max_val = self.df.loc[p, 'max']\n",
    "\n",
    "            # Replace missing values with default minimum and maximum values\n",
    "            if np.isnan(min_val):\n",
    "                min_val = None\n",
    "            if np.isnan(max_val):\n",
    "                max_val = None\n",
    "\n",
    "            # Append to bounds list\n",
    "            bnds.append((min_val, max_val))\n",
    "        return bnds\n",
    "\n",
    "    def initial(self):\n",
    "        \"\"\"Returns a numpy.ndarray of initial values for use in scipy.optimize.minimize\"\"\"\n",
    "        return self.df.loc[self.freeIdx(), 'value'].values.copy()\n",
    "\n",
    "    def index(self, p):\n",
    "        \"\"\"Returns the index of a given parameter in the parameter vector\"\"\"\n",
    "        idx = np.flatnonzero(self.df.index.values == p)\n",
    "        if len(idx) == 0:\n",
    "            raise KeyError(\"Unknown parameter name: {}\".format(p))\n",
    "        return idx[0]\n",
    "\n",
    "    def names(self, onlyFree=True):\n",
    "        \"\"\"Returns an array of the parameter names.\n",
    "\n",
    "        If `onlyFree == True`, only free parameters are returned.\n",
    "        Else, all parameters (including independent and fixed parameters) are returned.\"\"\"\n",
    "        if onlyFree:\n",
    "            return np.array(self.freeIdx(), dtype=np.object_)\n",
    "        else:\n",
    "            return self.df.index.values.copy()\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Returns a deep copy of this instance\"\"\"\n",
    "        return deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate models\n",
    "red_p = FitParameters(red, independent='t')\n",
    "red_p.set('G0r', min=0, value=G0r_0)\n",
    "red_p.set('Gu0r', min=0, value=Gu0r_0)\n",
    "red_p.set('betr', min=0, value=betr_0)\n",
    "red_p.set('kmr', min=0, value=kmr_0)\n",
    "\n",
    "green_p = FitParameters(green, independent='t')\n",
    "green_p.set('G0g', min=0, value=G0g_0)\n",
    "green_p.set('Gu0g', min=0, value=Gu0g_0)\n",
    "green_p.set('betg', min=0, value=betg_0)\n",
    "green_p.set('kmg', min=0, value=kmg_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "To increase the efficiency of fitting, the Jacobian matrix of the objective function is provided to the optimization routine.\n",
    "If the objective function is a typical negative log-likelihood function with normal distribution of residuals\n",
    "$$\n",
    " L(\\theta) = \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f(t\\mid\\theta)\\big)^2 \\text{,}\n",
    "$$\n",
    "where $D_t$ is the measured data at time $t$ and $f(t\\mid\\theta)$ is the value of the model function at time $t$ with parameters $\\theta$, the Jacobian is:\n",
    "$$\\begin{align}\n",
    "\\nabla L(\\theta) &= \\nabla \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\nabla \\frac{1}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{2}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\nabla\\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)\\big(\\nabla D_t - \\nabla  f\\left(t\\,\\middle|\\,\\theta\\right)\\big) \\\\\n",
    "&= -\\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big) \\nabla f\\left(t\\,\\middle|\\,\\theta\\right) \\\\\n",
    "\\end{align}$$\n",
    "We see that for calculating the Jacobian of the objective function we need the Jacobian of the model function.\n",
    "\n",
    "We use the general fixed expression model $G(t)$ function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian $\\nabla G\\left(t \\,\\middle|\\, G_0, G_{u0}, \\beta, k_m\\right)$ of the general fixed expression model function is the vector of the derivatives with respect to the various parameters:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial G}{\\partial G_0} &= \\mathrm{e}^{-\\beta t} \\\\\n",
    "\\frac{\\partial G}{\\partial G_{u0}} &= \\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\frac{\\partial G}{\\partial \\beta} &= -G_0 t \\mathrm{e}^{-\\beta t} - G_{u0} t \\left( \\mathrm{e}^{-(\\beta+k_m)t} \\right) = -t G(t)\\\\\n",
    "\\frac{\\partial G}{\\partial k_m} &= G_{u0} t \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_jacobian(t, G0, Gu0, beta, km):\n",
    "    \"\"\"Returns the Jacobi matrix of the general fixed expression model function\n",
    "    with time along axis=0 and parameters along axis=1\"\"\"\n",
    "\n",
    "    # Initialize Jacobian\n",
    "    jac = np.zeros((np.size(t), 3))\n",
    "\n",
    "    # Define abbreviations for frequent terms\n",
    "    ebt = np.exp(-beta * t)\n",
    "    ebkt = np.exp(-(beta + km) * t)\n",
    "\n",
    "    # Derive w.r.t. G0\n",
    "    #jac[:, 0] = ebt\n",
    "\n",
    "    # Derive w.r.t. Gu0\n",
    "    jac[:, 0] = ebt - ebkt\n",
    "\n",
    "    # Derive w.r.t. beta\n",
    "    jac[:, 1] = -t * (G0 * ebt + Gu0 * (ebt - ebkt))\n",
    "\n",
    "    # Derive w.r.t. km\n",
    "    jac[:, 2] = Gu0 * t * ebkt\n",
    "\n",
    "    return jac\n",
    "\n",
    "def red_jacobian(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Wrapper function for Jacobian of red model function\"\"\"\n",
    "    return general_jacobian(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green_jacobian(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Wrapper function for Jacobian of green model function\"\"\"\n",
    "    return general_jacobian(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian\n",
    "Analogously, the Hessian matrix is defined for better fit results:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial^2 L}{\\partial\\theta_2\\partial\\theta_1} &= \\frac{\\partial^2}{\\partial\\theta_2\\partial\\theta_1} \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{\\partial^2}{\\partial\\theta_2\\partial\\theta_1} \\frac{1}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{2}{2\\sigma_t^2} \\frac{\\partial}{\\partial\\theta_2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\frac{\\partial}{\\partial\\theta_1} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\frac{\\partial}{\\partial\\theta_2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\left(-\\frac{\\partial f(t)}{\\partial\\theta_1}\\right) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2}\n",
    "\\left( \\frac{\\partial f(t)}{\\partial\\theta_2} \\frac{\\partial f(t)}{\\partial\\theta_1} - \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\frac{\\partial^2 f(t)}{\\partial\\theta_2\\partial\\theta_1} \\right) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "The second order derivatives of the general fixed expression model function are:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0}^2} &= 0 \\\\\n",
    "\\frac{\\partial^2 G}{\\partial \\beta^2} &= G_0 t^2 \\mathrm{e}^{-\\beta t} + G_{u0} t^2 \\left( \\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t} \\right) \\\\\n",
    "\\frac{\\partial^2 G}{\\partial k_m^2} &= -G_{u0} t^2 \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0} \\partial \\beta} &= t \\left( \\mathrm{e}^{-(\\beta+k_m)t} - \\mathrm{e}^{-\\beta t} \\right) \\\\\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0} \\partial k_m} &= t \\mathrm{e}^{-(\\beta+k_m)t} \\\\\n",
    "\\frac{\\partial^2 G}{\\partial \\beta \\partial k_m} &= -G_{u0} t^2 \\mathrm{e}^{-(\\beta+k_m)t}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_hessian(t, G0, Gu0, beta, km):\n",
    "    \"\"\"Returns the Hessian matrix of the general fixed expression model function\n",
    "    with time along axis=0 and parameters along axis=1\"\"\"\n",
    "\n",
    "    # Initialize Hessian\n",
    "    hes = np.zeros((np.size(t), 6))\n",
    "\n",
    "    # Define abbreviations for frequent terms\n",
    "    ebt = np.exp(-beta * t)\n",
    "    ebkt = np.exp(-(beta + km) * t)\n",
    "    t2 = t**2\n",
    "\n",
    "    # Derive w.r.t. Gu0\n",
    "    # hes[:, 0] equals 0, do nothing\n",
    "\n",
    "    # Derive w.r.t. beta\n",
    "    hes[:, 1] = G0 * t2 * ebt + Gu0 * t2 * (ebt - ebkt)\n",
    "\n",
    "    # Derive w.r.t. km\n",
    "    hes[:, 2] = -Gu0 * t2 * ebkt\n",
    "\n",
    "    # Derive w.r.t. Gu0 and beta\n",
    "    hes[:, 3] = t * (ebkt - ebt)\n",
    "\n",
    "    # Derive w.r.t. Gu0 and km\n",
    "    hes[:, 4] = t * ebkt\n",
    "\n",
    "    # Derive w.r.t. beta and km\n",
    "    hes[:, 5] = -Gu0 * t2 * ebkt\n",
    "\n",
    "    return hes\n",
    "\n",
    "def red_hessian(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Wrapper function for Hessian of red model function\"\"\"\n",
    "    return general_hessian(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green_hessian(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Wrapper function for Hessian of green model function\"\"\"\n",
    "    return general_hessian(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and prepare result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate kernel density estimation of parameter distributions\n",
    "def parameter_KDE(par_tab, bw_div=15, dens_res=200, nice_ends=True):\n",
    "    \"\"\"\n",
    "    Returns a kernel density estimation of the parameter values for plotting.\n",
    "\n",
    "    Input parameters:\n",
    "        par_tab: pandas.DataFrame with parameters as columns and realizations as rows\n",
    "        bw_div: ratio data range – bandwidth; scalar or list with one entry per column\n",
    "        dens_res: sample number returned; scalar or list with one entry per column\n",
    "        nice_ends: (optional) if `True`, the start and end points will be set to 0\n",
    "\n",
    "    Returns:\n",
    "        dict with parameter names as keys. The values are dicts with keys \"val\" and \"prob\".\n",
    "        \"val\" contains an array of parameter values.\n",
    "        \"prob\" contains an array of the same length as for \"val\", containing the\n",
    "        probabilities/relative \n",
    "    \"\"\"\n",
    "    # Test lendth of `bw_div`\n",
    "    if not hasattr(bw_div, '__len__'):\n",
    "        this_bw_div = bw_div\n",
    "        has_multiple_bw_div = False\n",
    "    elif len(bw_div) == len(par_tab.columns):\n",
    "        has_multiple_bw_div = True\n",
    "    else:\n",
    "        raise ValueError(\"bw_div has length {:d}, but it must be scalar \"\n",
    "                        \"or a list with {:d} entries (one per column).\".format(\n",
    "                        len(dw_div), len(par_tab.columns)))\n",
    "\n",
    "    # Test length of `dens_res`\n",
    "    if not hasattr(dens_res, '__len__'):\n",
    "        this_dens_res = dens_res\n",
    "        has_multiple_dens_res = False\n",
    "    elif len(dens_res) == len(par_tab.columns):\n",
    "        has_multiple_dens_res = True\n",
    "    else:\n",
    "        raise ValueError(\"dens_res has length {:d}, but it must be scalar \"\n",
    "                        \"or a list with {:d} entries (one per column).\".format(\n",
    "                        len(dens_res), len(par_tab.columns)))\n",
    "\n",
    "    # Initialize return dictionary\n",
    "    par_dist = {}\n",
    "\n",
    "    for i, par_name in enumerate(par_tab.columns):\n",
    "        # Get parameter values\n",
    "        par_vals = par_tab.loc[:,par_name].values\n",
    "        par_vals = par_vals.reshape((-1, 1))\n",
    "\n",
    "        if has_multiple_dens_res:\n",
    "            this_dens_res = dens_res[i]\n",
    "        if has_multiple_bw_div:\n",
    "            this_bw_div = bw_div[i]\n",
    "\n",
    "        # Test parameter values for validity\n",
    "        if np.any(np.logical_not(np.isfinite(par_vals))):\n",
    "            print(\"Warning: invalid values encountered for “{}”\".format(par_name))\n",
    "            par_vals = par_vals(np.isfinite(par_vals))\n",
    "            if par_vals.size > 0:\n",
    "                # Reshape valid entries for KDE fit\n",
    "                par_vals = par_vals.reshape((-1, 1))\n",
    "            else:\n",
    "                # No valid entries found; cancel distribution calculation\n",
    "                par_dist[par_name] = {'val': [], 'prob': []}\n",
    "                continue\n",
    "\n",
    "        # Get parameter extrema and bandwidth\n",
    "        par_min = np.min(par_vals)\n",
    "        par_max = np.max(par_vals)\n",
    "        bw = (par_max - par_min) / this_bw_div\n",
    "\n",
    "        # Get kernel density estimation of parameter values\n",
    "        kde = KernelDensity(kernel='epanechnikov', bandwidth=bw).fit(par_vals)\n",
    "        par_x = np.linspace(par_min, par_max, this_dens_res).reshape((-1, 1))\n",
    "        par_dens = np.exp(kde.score_samples(par_x))\n",
    "\n",
    "        # Adjust values for nicer plotting (KDE >= 0, edges == 0)\n",
    "        #par_dens[par_dens < 0] = 0\n",
    "        if par_dens[0] != 0:\n",
    "            par_dens = np.insert(par_dens, 0, 0)\n",
    "            par_x = np.insert(par_x, 0, par_min)\n",
    "        if par_dens[-1] != 0:\n",
    "            par_dens = np.append(par_dens, 0)\n",
    "            par_x = np.append(par_x, par_max)\n",
    "\n",
    "        # Insert KDE into dict\n",
    "        par_dist[par_name] = {'val': par_x.flatten(), 'prob': par_dens.flatten()}\n",
    "    return par_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kde(ax, dist, label, clr_face='b', clr_edge='k', mark=None):\n",
    "    \"\"\"Plots the current parameter value in relation to the distribution\n",
    "    in the whole dataset.\"\"\"\n",
    "    ax.fill_betweenx(dist['val'], dist['prob'], color=clr_face)\n",
    "    if mark != None:\n",
    "        ax.axhline(y=mark, color=clr_edge)\n",
    "    ax.set_xticks([])\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    for s in [ax.spines[pos] for pos in ['bottom', 'right', 'top']]:\n",
    "        s.set_visible(False)\n",
    "    ax.set_title(label)\n",
    "    #ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:.2g}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loading\n",
    "\n",
    "# Define available files\n",
    "datafiles = [\n",
    "    {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq6\",\n",
    "        \"file\": \"data/2017-09-08_seq6_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq7\",\n",
    "        \"file\": \"data/2017-09-08_seq7_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq8\",\n",
    "        \"file\": \"data/2017-09-08_seq8_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq9\",\n",
    "        \"file\": \"data/2017-09-08_seq9_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq10\",\n",
    "        \"file\": \"data/2017-09-08_seq10_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq11\",\n",
    "        \"file\": \"data/2017-09-08_seq11_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq6\",\n",
    "        \"file\": \"data/2017-08-18_seq6_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq7\",\n",
    "        \"file\": \"data/2017-08-18_seq7_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq8\",\n",
    "        \"file\": \"data/2017-08-18_seq8_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq9\",\n",
    "        \"file\": \"data/2017-08-18_seq9_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq10\",\n",
    "        \"file\": \"data/2017-08-18_seq10_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18_seq11\",\n",
    "        \"file\": \"data/2017-08-18_seq11_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# By default, mark all files for loading\n",
    "load_idcs = range(len(datafiles))\n",
    "\n",
    "# Define function for loading data\n",
    "def load_data_from_files():\n",
    "    \"\"\"Loads data from specified files into `D`.\n",
    "    Requires `load_idcs` to hold a list of indices to `datafiles`.\"\"\"\n",
    "    global D\n",
    "    D = []\n",
    "    for i in load_idcs:\n",
    "        # Show message\n",
    "        print(\"Loading file: {}\".format(datafiles[i][\"file\"]))\n",
    "\n",
    "        # Read sheets from excel file\n",
    "        X = pd.read_excel(datafiles[i]['file'], dtype=np.float64, sheet_name=[\n",
    "            '#RFP', '#GFP', '#RFP_error', '#GFP_error'])\n",
    "\n",
    "        # Write data into easy-to-access structure\n",
    "        d = {}\n",
    "        d['sample'] = datafiles[i]['sample']\n",
    "        d['condition'] = datafiles[i]['condition']\n",
    "        d['measurement'] = datafiles[i]['measurement']\n",
    "        d['file'] = datafiles[i]['file']\n",
    "\n",
    "        if d['condition'] == 'gfp':\n",
    "            d['t'] = X['#GFP'].values[:,0].flatten()\n",
    "        else:\n",
    "            d['t'] = X['#RFP'].values[:,0].flatten()\n",
    "        #d['rfp'] = X['RFP'].values[:,1:]\n",
    "        #d['gfp'] = X['GFP_corrected'].values[:,1:]\n",
    "        d['rfp'] = X['#RFP'].values[:,1:]\n",
    "        d['gfp'] = X['#GFP'].values[:,1:]\n",
    "        d['rfp_error'] = X['#RFP_error'].values[:,1:]\n",
    "        d['gfp_error'] = X['#GFP_error'].values[:,1:]\n",
    "        D.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataLabel(i, filename=False):\n",
    "    \"\"\"Returns a nicely formatted name for the `i`-th element of `D`.\n",
    "    Set `filename=True` for a filename-friendly output.\"\"\"\n",
    "    if filename:\n",
    "        return \"{0[measurement]}_{0[sample]}_{0[condition]}\".format(D[i])\n",
    "    return \"{0[sample]}: {0[condition]} [{0[measurement]}]\".format(D[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data from excel sheets\n",
    "\n",
    "# Prompt user for files to load\n",
    "lbl = wdg.Label('Select the files to load:')\n",
    "lbl.layout.width = 'initial'\n",
    "entries = []\n",
    "for f in datafiles:\n",
    "    entries.append(\"{} {}: {}\".format(\n",
    "        f['sample'], f['condition'], f['file']))\n",
    "sel_entry = wdg.SelectMultiple(options=entries, rows=len(entries))\n",
    "sel_entry.layout.width = 'initial'\n",
    "bload = wdg.Button(description='Load')\n",
    "bselall = wdg.Button(description='Select all')\n",
    "bselnone = wdg.Button(description='Select none')\n",
    "\n",
    "# Define callbacks\n",
    "def sel_all_files(_):\n",
    "    sel_entry.value = entries\n",
    "def sel_no_files(_):\n",
    "    sel_entry.value = ()\n",
    "def load_button_clicked(_):\n",
    "    global load_idcs\n",
    "    load_idcs = [entries.index(r) for r in sel_entry.value]\n",
    "    vb.close()\n",
    "    load_data_from_files()\n",
    "bselall.on_click(sel_all_files)\n",
    "bselnone.on_click(sel_no_files)\n",
    "bload.on_click(load_button_clicked)\n",
    "\n",
    "# Finally, show the widgets\n",
    "vb = wdg.VBox((lbl, sel_entry, wdg.HBox((bload,bselall,bselnone))))\n",
    "IPython.display.display(vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide output tables\n",
    "\n",
    "# Initialize result dictionary\n",
    "R = []\n",
    "\n",
    "# Get a list of fit parameters\n",
    "#par_names = red_p.names().tolist()\n",
    "#par_names.extend(p for p in red_p.names() if p not in par_names)\n",
    "#par_names.sort()\n",
    "\n",
    "# Iteratively populate the result dictionary\n",
    "for k in range(len(D)):\n",
    "    R.insert(k, {})\n",
    "\n",
    "    cond = D[k]['condition']\n",
    "    if cond == 'rfp':\n",
    "        cols = red_p.names()\n",
    "    elif cond == 'gfp':\n",
    "        cols = green_p.names()\n",
    "\n",
    "    nTraces = np.shape(D[k][cond])[1]\n",
    "    nTimes = np.shape(D[k][cond])[0]\n",
    "    tpl_traces = np.empty((nTimes, nTraces))\n",
    "    tpl_traces.fill(np.NaN)\n",
    "\n",
    "    R[k][cond] = {}\n",
    "    R[k][cond]['params'] = pd.DataFrame(index=np.arange(nTraces), columns=cols, dtype='float64')\n",
    "    R[k][cond]['fit'] = np.copy(tpl_traces)\n",
    "    R[k][cond]['success'] = np.zeros(nTraces, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle or load fitting results\n",
    "Pickling is only reasonable if the result list `R` has already been populated by fitting (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle fit results for future sessions\n",
    "outfile = getOutpath('fixed_fit_results.pickled')\n",
    "with open(outfile, 'wb') as f:\n",
    "    pickle.dump(R, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pickled results (requires file suffix “.pickled”)\n",
    "pickfiles = [f for f in os.listdir(getOutpath()) if f.lower().endswith('.pickled')]\n",
    "pickfiles.sort(reverse=True)\n",
    "\n",
    "lbl = wdg.Label('Select the file to load:')\n",
    "lbl.layout.width = 'initial'\n",
    "rad = wdg.RadioButtons(options=pickfiles)\n",
    "but = wdg.Button(description='Load')\n",
    "vb = wdg.VBox([lbl, rad, but])\n",
    "IPython.display.display(vb)\n",
    "\n",
    "def clicked_on_but(b):\n",
    "    global R\n",
    "    with open(getOutpath(rad.value, ''), 'rb') as f:\n",
    "        R = pickle.load(f)\n",
    "    print('Loaded: ' + rad.value)\n",
    "    vb.close()\n",
    "but.on_click(clicked_on_but)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to XLSX\n",
    "if len(R) != len(D):\n",
    "    raise ValueError(\"R and D must have the same length!\")\n",
    "\n",
    "samples = set()\n",
    "conditions = set()\n",
    "\n",
    "for k in range(len(D)):\n",
    "    # Collect information about this file\n",
    "    sample = D[k]['sample']\n",
    "    condition = D[k]['condition']\n",
    "    measurement = D[k]['measurement']\n",
    "    time = D[k]['t']\n",
    "\n",
    "    if condition == 'rfp':\n",
    "        rfp_raw = D[k]['rfp']\n",
    "        rfp_error = D[k]['rfp_error']\n",
    "        rfp_fit = R[k]['rfp']['fit']\n",
    "        rfp_params = R[k]['rfp']['params']\n",
    "    elif condition == 'gfp':\n",
    "        gfp_raw = D[k]['gfp']\n",
    "        gfp_error = D[k]['gfp_error']\n",
    "        gfp_fit = R[k]['gfp']['fit']\n",
    "        gfp_params = R[k]['gfp']['params']\n",
    "\n",
    "    # Write data to file\n",
    "    file = getOutpath(\"CHX__{}_{}_{}.xlsx\".format(sample, measurement, condition))\n",
    "    xlsx_writer = pd.ExcelWriter(file, engine='xlsxwriter')\n",
    "\n",
    "    pd.DataFrame(time).to_excel(xlsx_writer, sheet_name=\"t\")\n",
    "\n",
    "    if condition == 'rfp':\n",
    "        pd.DataFrame(rfp_raw).to_excel(xlsx_writer, sheet_name=\"RFP_raw\")\n",
    "        pd.DataFrame(rfp_error).to_excel(xlsx_writer, sheet_name=\"RFP_error\")\n",
    "        pd.DataFrame(rfp_fit).to_excel(xlsx_writer, sheet_name=\"RFP_fit\")\n",
    "        rfp_params.to_excel(xlsx_writer, sheet_name=\"RFP_params\")\n",
    "    elif condition == 'gfp':\n",
    "        pd.DataFrame(gfp_raw).to_excel(xlsx_writer, sheet_name=\"GFP_raw\")\n",
    "        pd.DataFrame(gfp_error).to_excel(xlsx_writer, sheet_name=\"GFP_error\")\n",
    "        pd.DataFrame(gfp_fit).to_excel(xlsx_writer, sheet_name=\"GFP_fit\")\n",
    "        gfp_params.to_excel(xlsx_writer, sheet_name=\"GFP_params\")\n",
    "    \n",
    "    xlsx_writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and plot separate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSeparate(ds, tr, pdf=None, par_kde=None):\n",
    "    \"\"\"Fits and plots the data, treating RFP and GFP separately.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ds -- the dictionary key of the dataset\n",
    "    tr -- the index of the trace in the dataset to be processed\n",
    "    pdf -- a PdfPages object to which the figure is written if it is not None\n",
    "    par_kde -- if containing dict of values of parameter distributions, plot distributions\n",
    "    \"\"\"\n",
    "\n",
    "    # Get trace information\n",
    "    cond = D[ds]['condition']\n",
    "\n",
    "    # Plot fit results\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if par_kde != None:\n",
    "        fig.set_figwidth(1.6 * fig.get_figwidth())\n",
    "\n",
    "        if cond == 'rfp':\n",
    "            pn = ['Gu0r', 'betr', 'kmr']\n",
    "            clr_face = '#ff000055'\n",
    "            clr_edge = '#990000ff'\n",
    "        elif cond == 'gfp':\n",
    "            pn = ['Gu0g', 'betg', 'kmg']\n",
    "            clr_face = '#00ff0055'\n",
    "            clr_edge = '#009900ff'\n",
    "\n",
    "        grid = (1, len(pn))\n",
    "        gs = GridSpec(grid[0], grid[1])\n",
    "\n",
    "        # Plot parameters\n",
    "        for pi, label in enumerate(pn):\n",
    "            ax = fig.add_subplot(gs[0, pi])\n",
    "            data = par_kde[label]\n",
    "            curr_val = R[ds][cond]['params'].loc[tr,label]\n",
    "            plot_kde(ax, data, label, clr_face, clr_edge, curr_val)\n",
    "\n",
    "        # Adjust subplot layout\n",
    "        gs.tight_layout(fig, pad=0, rect=(0.5, 0, 1, 1))\n",
    "\n",
    "        # Create axes for fit\n",
    "        gs_fit = GridSpec(1, 1)\n",
    "        ax = fig.add_subplot(gs_fit[0])\n",
    "        gs_fit.tight_layout(fig, pad=0, rect=(0, 0, 0.5, 1))\n",
    "\n",
    "    else:\n",
    "        ax = fig.gca()\n",
    "\n",
    "    if cond == 'rfp':\n",
    "        p_f, = ax.plot(D[ds]['t'], R[ds][cond]['fit'][:,tr], '-', label='RFP (fit)', color='#ff0000', linewidth=1)\n",
    "        p_d, = ax.plot(D[ds]['t'], D[ds][cond][:,tr], '-', label='RFP (measured)', color='#990000', linewidth=.5)\n",
    "    elif cond == 'gfp':\n",
    "        p_f, = ax.plot(D[ds]['t'], R[ds][cond]['fit'][:,tr], '-', label='GFP (fit)', color='#00ff00', linewidth=1)\n",
    "        p_d, = ax.plot(D[ds]['t'], D[ds][cond][:,tr], '-', label='GFP (measured)', color='#009900', linewidth=.5)\n",
    "\n",
    "    # Format plot\n",
    "    ax.set_xlabel('Time [h]')\n",
    "    ax.set_ylabel('Number of molecules [10³]')\n",
    "    ax.set_title('{} #{:03d}\\n(separate fit)'.format(getDataLabel(ds), tr))\n",
    "    ax.legend(handles=[p_d, p_f])\n",
    "\n",
    "    # Write figure to pdf\n",
    "    if pdf != None:\n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "\n",
    "    # Show and close figure\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit traces separately\n",
    "for ds in range(len(D)):\n",
    "\n",
    "    # Define data-dependent functions and parameters\n",
    "    cond = D[ds]['condition']\n",
    "    if cond == 'rfp':\n",
    "        ftprm = red_p\n",
    "        mdl_fcn = red\n",
    "        jac_fcn = red_jacobian\n",
    "        hes_fcn = red_hessian\n",
    "    elif cond == 'gfp':\n",
    "        ftprm = green_p\n",
    "        mdl_fcn = green\n",
    "        jac_fcn = green_jacobian\n",
    "        hes_fcn = green_hessian\n",
    "\n",
    "    nTraces = np.shape(D[ds][cond])[1]\n",
    "\n",
    "    for tr in range(nTraces):\n",
    "        print('Fitting „{}“ #{:03d}/{:03d} …'.format(getDataLabel(ds), tr, nTraces))\n",
    "        \n",
    "        # Prepare data\n",
    "        time = D[ds]['t']\n",
    "        data = D[ds][cond][:,tr].flatten()\n",
    "        wght = 1 / D[ds][cond + '_error'][:,tr]**2\n",
    "\n",
    "        # Prepare parameters\n",
    "        ftprm.set('G0' + cond[0], fixed=True, value=data[0])\n",
    "        ftprm.set('Gu0' + cond[0], value=.5*(data.max() - data.min()))\n",
    "\n",
    "        # Objective function (closure)\n",
    "        def objective_fcn(params):\n",
    "            \"\"\"Objective function for separate model\"\"\"\n",
    "            cur_val = ftprm.eval(params, t=time)\n",
    "            chisq = np.sum((data - cur_val)**2 * wght)\n",
    "            return chisq\n",
    "\n",
    "        # Jacobian/gradient (closure)\n",
    "        def gradient_fcn(params):\n",
    "            \"\"\"Gradient for separate model\"\"\"\n",
    "            J = jac_fcn(**ftprm.eval_params(params, t=time))\n",
    "            residuals = (data - ftprm.eval(params, t=time)).reshape((np.size(time),1))\n",
    "            vrnc = wght.reshape(np.shape(residuals))\n",
    "            return -np.sum(J * residuals * vrnc, axis=0).flatten()\n",
    "\n",
    "        # Hessian/second derivative (closure)\n",
    "        #def secderiv_fcn(params):\n",
    "        #    \"\"\"Hessian for separate model\"\"\"\n",
    "        #    H = hes_fcn(**ftprm.eval_params(params, t=time))\n",
    "        #    residuals = (data - ftprm.eval(params, t=time)).reshape((np.size(time), 1))\n",
    "        #    jac = jac_fcn(**ftprm.eval_params(params, t=time))\n",
    "        #    vrnc = wght.reshape((np.size(time), 1, 1))\n",
    "\n",
    "        #    H *= residuals\n",
    "        #    hes = np.empty((np.size(time), 3, 3))\n",
    "        #    for i in range(3):\n",
    "        #        hes[:,i,i] = jac[:,i]**2 - H[:,i]\n",
    "        #    hes[:,0,1] = jac[:,0] * jac[:,1] - H[:,3]\n",
    "        #    hes[:,0,2] = jac[:,0] * jac[:,2] - H[:,4]\n",
    "        #    hes[:,1,2] = jac[:,1] * jac[:,2] - H[:,5]\n",
    "        #    hes[:,1,0] = hes[:,0,1]\n",
    "        #    hes[:,2,0] = hes[:,0,2]\n",
    "        #    hes[:,2,1] = hes[:,1,2]\n",
    "        #    hes *= vrnc\n",
    "\n",
    "        #    return np.sum(hes, axis=0)\n",
    "\n",
    "        # Fit the data\n",
    "        result = sc.optimize.minimize(objective_fcn,\n",
    "                                      ftprm.initial(),\n",
    "                                      method='TNC',# one of: 'SLSQP' 'TNC' 'L-BFGS-B'\n",
    "                                      bounds=ftprm.bounds(),\n",
    "                                      jac=gradient_fcn,\n",
    "                                      #hess=secderiv_fcn,\n",
    "                                      options={'disp':True,\n",
    "                                               'maxiter': 20000}\n",
    "                                     )\n",
    "        #result = sc.optimize.least_squares(\n",
    "        #    objective_fcn,\n",
    "        #    ftprm.initial(),\n",
    "        #    jac=gradient_fcn,\n",
    "        #    bounds=(0, np.inf),\n",
    "        #    max_nfev=20000\n",
    "        #)\n",
    "\n",
    "        # Print result\n",
    "        print(\"\\tSuccess {}: {}\".format(result.success, result.message))\n",
    "\n",
    "        # Save results to R\n",
    "        R[ds][cond]['params'].iloc[tr] = ftprm.eval_params(result.x, independent=False)\n",
    "        best_fit = ftprm.eval(result.x, t=time)\n",
    "        #best_fit = mdl_fcn(time, *result.x)\n",
    "        R[ds][cond]['fit'][:,tr] = best_fit\n",
    "        R[ds][cond]['success'][tr] = result.success\n",
    "\n",
    "        # DEBUG\n",
    "        #if tr >= 2:\n",
    "        #    print(\"Breaking loop for debugging purposes\")\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results of separate fit\n",
    "ts = getTimeStamp()\n",
    "\n",
    "for ds in range(len(D)):\n",
    "    cond = D[ds]['condition']\n",
    "    par_kde = parameter_KDE(R[ds][cond]['params'])\n",
    "    pdffile = getOutpath('separate_{}.pdf'.format(getDataLabel(ds, True)))\n",
    "    with PdfPages(pdffile) as pdf:\n",
    "        for tr in range(np.shape(D[ds][cond])[1]):\n",
    "            plotSeparate(ds, tr, pdf, par_kde)\n",
    "\n",
    "            # DEBUG\n",
    "            #if tr >= 2:\n",
    "            #    print(\"Break loop\")\n",
    "            #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify ill-shaped raw traces\n",
    "For these measurements, the parts before CHX addition were cut away from the traces.\n",
    "While most traces now start with an ascend, some traces have minima after the first value.\n",
    "As our fitting model does not account for this behaviour, such traces may be fitted badly and affect the mean parameter values.\n",
    "\n",
    "We therefore want to identify those traces and sort them out.\n",
    "\n",
    "A trace is treated as ill-shaped if its smallest value before the maximum is\n",
    "* the third or a later value or\n",
    "* the second value and\n",
    "  * its difference to the first value is larger than 3% of its difference to the maximum or\n",
    "  * another value before the maximum is smaller than the first value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_traces(data):\n",
    "    \"\"\"\n",
    "    Finds ill-shaped traces. A trace is ill-shaped or bad if\n",
    "    any value before the maximum is smaller than the first value.\n",
    "    However, if only the second value is smaller than the first value\n",
    "    and the difference between first and second value does not exceed\n",
    "    a threshold (3%) of the difference between maximum value and\n",
    "    minimum value, the trace is not assumed bad.\n",
    "\n",
    "    Argument:\n",
    "        data -- array of traces (columns: traces, row: timepoints)\n",
    "\n",
    "    Returns:\n",
    "        1-dim indexing array, where the i-th element indicates if\n",
    "        the i-th trace from data is bad (True) or not (False).\n",
    "    \"\"\"\n",
    "    # Find maxima and minima of the traces\n",
    "    maxima = data.argmax(axis=0)\n",
    "    minima = np.zeros_like(maxima)\n",
    "    for i, m in enumerate(maxima):\n",
    "        minima[i] = data[:m,i].argmin(axis=0)\n",
    "\n",
    "    # Mark all traces as bad where the first value is not the minimum\n",
    "    bad_traces = minima > 0\n",
    "\n",
    "    for i, m in enumerate(minima):\n",
    "        # If only the second value is smaller than the first …\n",
    "        if (m == 1) and data[2:maxima[i],i].min() > data[0,i]:\n",
    "            # … check if the relative difference exceeds a threshold …\n",
    "            amp = data[maxima[i],i] - data[m,i]\n",
    "            if (data[0,i] - data[1,i]) / amp <= 0.3:\n",
    "                # … and if not, do not discard the trace\n",
    "                bad_traces[i] = False\n",
    "\n",
    "    return bad_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for bad traces\n",
    "bad_traces_red = pd.DataFrame(False, index=chsq_red.index, columns=('bad',), dtype=np.bool_)\n",
    "bad_traces_green = pd.DataFrame(False, index=chsq_green.index, columns=('bad',), dtype=np.bool_)\n",
    "\n",
    "for i, d in enumerate(D):\n",
    "    condition = d['condition']\n",
    "    data = d[condition]\n",
    "\n",
    "    # Identify bad traces:\n",
    "    # the minimum is before the maximum, but not the first point\n",
    "    #minima = data.argmin(axis=0)\n",
    "    #maxima = data.argmax(axis=0)\n",
    "    #bad_traces = (minima > 0) & (minima < maxima)\n",
    "    bad_traces = find_bad_traces(data)\n",
    "\n",
    "    # Save bad traces in condition-specific table\n",
    "    if condition == 'rfp':\n",
    "        bad_traces_red.loc[i, 'bad'] = bad_traces\n",
    "    elif condition == 'gfp':\n",
    "        bad_traces_green.loc[i, 'bad'] = bad_traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bad traces\n",
    "with PdfPages(getOutpath(\"fixed_bad_traces.pdf\")) as pdf:\n",
    "    for condition in (\"rfp\", \"gfp\"):\n",
    "        if condition == 'rfp':\n",
    "            bad_traces = bad_traces_red\n",
    "        elif condition == 'gfp':\n",
    "            bad_traces = bad_traces_green\n",
    "\n",
    "        for i, j in bad_traces.index:\n",
    "            if not bad_traces.loc[(i,j),'bad']:\n",
    "                continue\n",
    "\n",
    "            clr = condition[0]\n",
    "            t = D[i]['t']\n",
    "\n",
    "            f, ax = plt.subplots(1, 1)\n",
    "            ax.plot(t, D[i][condition][:,j], '-', color=clr, lw=1, label=\"measured\")\n",
    "            ax.plot(t, R[i][condition]['fit'][:,j], '-k', lw=0.5, label=\"best fit\")\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"Time [h]\")\n",
    "            ax.set_ylabel(\"Fluorescence [a.u.]\")\n",
    "            ax.set_title(\"Trace {:03d} in {}\".format(j, getDataLabel(i)))\n",
    "\n",
    "            f.tight_layout(pad=0)\n",
    "            plt.show(f)\n",
    "            pdf.savefig(f)\n",
    "            plt.close(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-based data filtering\n",
    "The $\\chi^2$ values are examined. It is found that the $\\chi^2$ values are nicely distributed and that no satisfying correlation between $\\chi^2$ and outliers of parameter values exists. \n",
    "\n",
    "Hence, the idea of outlier filtering based on $\\chi^2$ is discarded.\n",
    "Instead, clustering-based outlier filtering is set up.\n",
    "\n",
    "The traces are clustered with respect to $\\beta$ and $k_\\mathrm{m}$ using DBSCAN.\n",
    "All traces that do not belong to the largest cluster are assumed to be outliers.\n",
    "The center-of-mass of the largest cluster is written into an XLSX file for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquare(i_data, i_trace, to_dict=False, to_dataframe=False):\n",
    "    \"\"\"\n",
    "    Computes the chi-square value of the best fit for a given trace\n",
    "\n",
    "    Arguments:\n",
    "        i_data -- index of the dataset in D or R\n",
    "        i_trace -- index of the trace in the dataset\n",
    "\n",
    "    Returns:\n",
    "        chi-square value of given trace as scalar value\n",
    "    \"\"\"\n",
    "    condition = D[i_data]['condition']\n",
    "    data = D[i_data][condition][:,i_trace]\n",
    "    t = D[i_data]['t']\n",
    "    params = R[i_data][condition]['params'].iloc[i_trace,:].to_dict()\n",
    "\n",
    "    if condition == 'rfp':\n",
    "        fit = red(t=t, **params)\n",
    "    elif condition == 'gfp':\n",
    "        fit = green(t=t, **params)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown condition: {}\".format(condition))\n",
    "\n",
    "    xq = np.sum((data - fit)**2)\n",
    "    if to_dict or to_dataframe:\n",
    "        xq_dict = {'chisq': xq, **params}\n",
    "        if to_dataframe:\n",
    "            return pd.DataFrame(xq_dict, index=(0,))\n",
    "        return xq_dict\n",
    "    return xq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tables of chi-square values for RFP and GFP\n",
    "muli = pd.MultiIndex(names=['Dataset', 'Trace'], levels=[[], []], labels=[[], []])\n",
    "chsq_red = pd.DataFrame([], index=muli, columns=('chisq', 'G0r', 'Gu0r', 'betr', 'kmr'),\n",
    "                        dtype=np.float_)\n",
    "chsq_green = pd.DataFrame([], index=muli, columns=('chisq', 'G0g', 'Gu0g', 'betg', 'kmg'),\n",
    "                          dtype=np.float_)\n",
    "\n",
    "for i_data, d in enumerate(D):\n",
    "\n",
    "    condition = d['condition']\n",
    "    if condition == 'rfp':\n",
    "        xq_tab = chsq_red\n",
    "    elif condition == 'gfp':\n",
    "        xq_tab = chsq_green\n",
    "    else:\n",
    "        raise ValueError(\"Unknown condition: {}\".format(condition))\n",
    "\n",
    "    for i_trace in range(d[condition].shape[1]):\n",
    "        xq_tab.loc[(i_data, i_trace),:] = chisquare(i_data, i_trace,\n",
    "                                                    to_dataframe=True).iloc[0,:]\n",
    "\n",
    "# Plot chi-square histograms\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.hist(np.log10(chsq_red.loc[:,'chisq'].values), bins=50, color='red')\n",
    "ax1.set_title(r\"$\\chi^2$ for RFP\")\n",
    "ax1.set_xlabel(r\"$\\log(\\chi^2)$ [a.u.]\")\n",
    "ax1.set_ylabel(\"Occurrences\")\n",
    "ax2.hist(np.log10(chsq_green.loc[:,'chisq'].values), bins=50, color='green')\n",
    "ax2.set_title(r\"$\\chi^2$ for GFP\")\n",
    "ax2.set_xlabel(r\"$\\log(\\chi^2)$ [a.u.]\")\n",
    "ax2.set_ylabel(\"Occurrences\")\n",
    "\n",
    "f.tight_layout(pad=0)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_chisquare_histograms.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chsq_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chsq_green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot parameter – chi-square correlations\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)\n",
    "\n",
    "# RFP: km\n",
    "ax1.plot(chsq_red.loc[:,'chisq'].values, chsq_red.loc[:,'kmr'].values, '.r', ms=2)\n",
    "ax1.set_ylabel(r\"$k_\\mathrm{m}$\")\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title(\"RFP\")\n",
    "\n",
    "# RFP: beta\n",
    "ax3.plot(chsq_red.loc[:,'chisq'].values, chsq_red.loc[:,'betr'].values, '.r', ms=2)\n",
    "ax3.set_xlabel(r\"$\\chi^2$\")\n",
    "ax3.set_ylabel(r\"$\\beta$\")\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# GFP: km\n",
    "ax2.plot(chsq_green.loc[:,'chisq'].values, chsq_green.loc[:,'kmg'].values, '.g', ms=2)\n",
    "ax2.set_ylabel(r\"$k_\\mathrm{m}$\")\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title(\"GFP\")\n",
    "\n",
    "# GFP: beta\n",
    "ax4.plot(chsq_green.loc[:,'chisq'].values, chsq_green.loc[:,'betg'].values, '.g', ms=2)\n",
    "ax4.set_xlabel(r\"$\\chi^2$\")\n",
    "ax4.set_ylabel(r\"$\\beta$\")\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "f.tight_layout(pad=0, w_pad=2)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_chisquare_correlations.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot beta – km correlations\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# RFP\n",
    "ax1.plot(chsq_red.loc[:,'betr'], chsq_red.loc[:,'kmr'], 'r.', ms=2)\n",
    "ax1.set_xlabel(r\"$\\beta$ [h${}^{-1}$]\")\n",
    "ax1.set_ylabel(r\"$k_\\mathrm{m}$ [h${}^{-1}$]\")\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title(\"RFP\")\n",
    "\n",
    "# GFP\n",
    "ax2.plot(chsq_green.loc[:,'betg'], chsq_green.loc[:,'kmg'], 'g.', ms=2)\n",
    "ax2.set_xlabel(r\"$\\beta$ [h${}^{-1}$]\")\n",
    "ax2.set_ylabel(r\"$k_\\mathrm{m}$ [h${}^{-1}$]\")\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title(\"GFP\")\n",
    "\n",
    "f.tight_layout(pad=0, w_pad=2)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_correlations.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_params(X, eps=0.15, min_samples=3, discard=None):\n",
    "    \"\"\"\n",
    "    Clusters data X with DBSCAN\n",
    "\n",
    "    Input:\n",
    "        X -- n_samples x n_dimensions array of data to be clustered\n",
    "        eps -- maximum distance parameter for DBSCAN\n",
    "        min_samples -- minimum neighborhood size parameter for DBSCAN\n",
    "        discard -- 1-dim logical indexing array of length n_samples with\n",
    "                   True for cells to be discarded and False for cells to be clustered\n",
    "\n",
    "    Returns:\n",
    "        1-dim logical indexing array indicating rows in X that belong to largest cluster\n",
    "    \"\"\"\n",
    "    # Prepare data (work in log-space)\n",
    "    Xlog = np.log10(X)\n",
    "    mask = np.all(np.isfinite(Xlog), axis=1)\n",
    "    if discard is not None:\n",
    "        mask = mask & ~discard\n",
    "    Xlog = Xlog[mask,:]\n",
    "\n",
    "    # Perform DBSCAN\n",
    "    dbscan = skc.DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(Xlog)\n",
    "\n",
    "    # Find largest cluster\n",
    "    lbls = set(dbscan.labels_) - {-1}\n",
    "    max_lbl_count = 0\n",
    "    max_lbl = None\n",
    "    for l in lbls:\n",
    "        n_this = (dbscan.labels_ == l).sum()\n",
    "        if n_this > max_lbl_count:\n",
    "            max_lbl_count = n_this\n",
    "            max_lbl = l\n",
    "\n",
    "    # Build list of samples in cluster\n",
    "    mask[mask] = dbscan.labels_ == max_lbl\n",
    "\n",
    "    return mask\n",
    "\n",
    "def center_of_mass(X):\n",
    "    \"\"\"\n",
    "    Returns the center of mass of the data X.\n",
    "\n",
    "    X is the data array with n_samples rows and n_dimensions columns.\n",
    "    A 1-dim array with n_dimensions elements is returned, where each\n",
    "    element is the mean value of the corresponding column in X.\n",
    "    \"\"\"\n",
    "    return np.power(10, np.log10(X).mean(axis=0))\n",
    "\n",
    "def std_dev(X):\n",
    "    \"\"\"\n",
    "    Returns the standard deviation of the data X.\n",
    "\n",
    "    X is the data array with n_samples rows and n_dimensions columns.\n",
    "    A 1-dim array with n_dimensions elements is returned, where each\n",
    "    element is the standard deviation of the corresponding column in X.\n",
    "    \"\"\"\n",
    "    #return np.power(10, np.log10(X).std(axis=0))\n",
    "    return np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering\n",
    "Xr = chsq_red.loc[:,('betr','kmr')].values\n",
    "Xg = chsq_green.loc[:,('betg','kmg')].values\n",
    "\n",
    "bad_r = bad_traces_red['bad'].values\n",
    "bad_g = bad_traces_green['bad'].values\n",
    "\n",
    "cluster_r = cluster_params(Xr, eps=0.09, discard=bad_r)\n",
    "cluster_g = cluster_params(Xg, eps=0.2, discard=bad_g)\n",
    "\n",
    "outliers_r = ~(cluster_r | bad_r)\n",
    "outliers_g = ~(cluster_g | bad_g)\n",
    "\n",
    "com_r = center_of_mass(Xr[cluster_r,:])\n",
    "com_g = center_of_mass(Xg[cluster_g,:])\n",
    "\n",
    "# Plot clustering result\n",
    "legend_opts = {'fontsize': 'small', 'borderpad': .3, 'labelspacing': .3,\n",
    "               'handletextpad': .3, 'handlelength': 1}\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(Xr[cluster_r,0], Xr[cluster_r,1], '.r', ms=2, label=\"accepted ({})\".format(cluster_r.sum()))\n",
    "ax1.plot(Xr[outliers_r,0], Xr[outliers_r,1], 'ok', mfc='none', ms=2, mew=0.5, label=\"outlier ({})\".format(outliers_r.sum()))\n",
    "ax1.plot(Xr[bad_r,0], Xr[bad_r,1], 'sk', mfc='none', ms=2, mew=0.5, label=\"bad trace ({})\".format(bad_r.sum()))\n",
    "ax1.plot(com_r[0], com_r[1], 'xk', label=\"mean\")\n",
    "ax1.legend(**legend_opts)\n",
    "ax1.set_xlabel(r\"$\\beta$ [h${}^{-1}$]\")\n",
    "ax1.set_ylabel(r\"$k_\\mathrm{m}$ [h${}^{-1}$]\")\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title(\"RFP\")\n",
    "\n",
    "ax2.plot(Xg[cluster_g,0], Xg[cluster_g,1], '.g', ms=2, label=\"accepted ({})\".format(cluster_g.sum()))\n",
    "ax2.plot(Xg[outliers_g,0], Xg[outliers_g,1], 'ok', mfc='none', ms=2, mew=0.5, label=\"outlier ({})\".format(outliers_g.sum()))\n",
    "ax2.plot(Xg[bad_g,0], Xg[bad_g,1], 'sk', mfc='none', ms=2, mew=0.5, label=\"bad trace ({})\".format(bad_g.sum()))\n",
    "ax2.plot(com_g[0], com_g[1], 'xk', label=\"mean\")\n",
    "ax2.legend(**legend_opts)\n",
    "ax2.set_xlabel(r\"$\\beta$ [h${}^{-1}$]\")\n",
    "ax2.set_ylabel(r\"$k_\\mathrm{m}$ [h${}^{-1}$]\")\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title(\"GFP\")\n",
    "\n",
    "f.tight_layout(pad=0, w_pad=2)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_correlations_cluster.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outlier traces\n",
    "with PdfPages(getOutpath(\"fixed_outlier_traces.pdf\")) as pdf:\n",
    "    for condition in (\"rfp\", \"gfp\"):\n",
    "        if condition == 'rfp':\n",
    "            outliers_idx = outliers_r\n",
    "            xq_tab = chsq_red\n",
    "        elif condition == 'gfp':\n",
    "            outliers_idx = outliers_g\n",
    "            xq_tab = chsq_green\n",
    "\n",
    "        for i, j in xq_tab.index[outliers_idx]:\n",
    "\n",
    "            clr = condition[0]\n",
    "            t = D[i]['t']\n",
    "\n",
    "            f, ax = plt.subplots(1, 1)\n",
    "            ax.plot(t, D[i][condition][:,j], '-', color=clr, lw=1, label=\"measured\")\n",
    "            ax.plot(t, R[i][condition]['fit'][:,j], '-k', lw=0.5, label=\"best fit\")\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"Time [h]\")\n",
    "            ax.set_ylabel(\"Fluorescence [a.u.]\")\n",
    "            ax.set_title(\"Trace {:03d} in {}\".format(j, getDataLabel(i)))\n",
    "\n",
    "            f.tight_layout(pad=0)\n",
    "            plt.show(f)\n",
    "            pdf.savefig(f)\n",
    "            plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write values to XLSX files\n",
    "\n",
    "# Build result tables\n",
    "cols = ('mean', 'std_dev', 'stat_total', 'stat_used')\n",
    "red_mean_tab = pd.DataFrame(index=chsq_red.columns[1:], columns=cols, dtype=np.float_)\n",
    "green_mean_tab = pd.DataFrame(index=chsq_green.columns[1:], columns=cols, dtype=np.float_)\n",
    "\n",
    "# Populate result tables\n",
    "red_mean_tab.loc[['betr','kmr'],'mean'] = center_of_mass(chsq_red.loc[:,['betr','kmr']].values[cluster_r,:])\n",
    "red_mean_tab.loc[['G0r','Gu0r'],'mean'] = np.mean(chsq_red.loc[:,['G0r','Gu0r']].values[cluster_r,:], axis=0)\n",
    "red_mean_tab.loc[:,'std_dev'] = std_dev(chsq_red.loc[:,['G0r','Gu0r','betr','kmr']].values[cluster_r,:])\n",
    "red_mean_tab.loc[:,'stat_total'] = cluster_r.size\n",
    "red_mean_tab.loc[:,'stat_used'] = cluster_r.sum()\n",
    "\n",
    "green_mean_tab.loc[['betg','kmg'],'mean'] = center_of_mass(chsq_green.loc[:,['betg','kmg']].values[cluster_g,:])\n",
    "green_mean_tab.loc[['G0g','Gu0g'],'mean'] = np.mean(chsq_green.loc[:,['G0g','Gu0g']].values[cluster_g,:], axis=0)\n",
    "green_mean_tab.loc[:,'std_dev'] = std_dev(chsq_green.loc[:,['G0g','Gu0g','betg','kmg']].values[cluster_g,:])\n",
    "green_mean_tab.loc[:,'stat_total'] = cluster_g.size\n",
    "green_mean_tab.loc[:,'stat_used'] = cluster_g.sum()\n",
    "\n",
    "# Save distribution parameters to spreadsheet\n",
    "xlsx_file = getOutpath(\"fixed_distribution_moments.xlsx\")\n",
    "xlsx_writer = pd.ExcelWriter(xlsx_file, engine='xlsxwriter')\n",
    "red_mean_tab.to_excel(xlsx_writer, sheet_name=\"red\", na_rep=\"NaN\")\n",
    "green_mean_tab.to_excel(xlsx_writer, sheet_name=\"green\", na_rep=\"NaN\")\n",
    "xlsx_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_mean_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "green_mean_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control: parameter – chi-square histograms for clustered data\n",
    "legend_opts = {'fontsize': 'small', 'borderpad': .3, 'labelspacing': .3,\n",
    "               'handletextpad': .3, 'handlelength': .5}\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex=True)\n",
    "\n",
    "# RFP: km\n",
    "ax1.plot(chsq_red['chisq'].values[cluster_r], chsq_red['kmr'].values[cluster_r], '.r',\n",
    "         ms=2, label=\"accepted ({})\".format(cluster_r.sum()))\n",
    "ax1.plot(chsq_red['chisq'].values[outliers_r], chsq_red['kmr'].values[outliers_r], 'ok',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"outlier ({})\".format(outliers_r.sum()))\n",
    "ax1.plot(chsq_red['chisq'].values[bad_r], chsq_red['kmr'].values[bad_r], 'sk',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"bad trace ({})\".format(bad_r.sum()))\n",
    "ax1.set_ylabel(r\"$k_\\mathrm{m}$\")\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_title(\"RFP\")\n",
    "ax1.legend(**legend_opts)\n",
    "\n",
    "# RFP: beta\n",
    "ax3.plot(chsq_red['chisq'].values[cluster_r], chsq_red['betr'].values[cluster_r], '.r',\n",
    "         ms=2, label=\"accepted ({})\".format(cluster_r.sum()))\n",
    "ax3.plot(chsq_red['chisq'].values[outliers_r], chsq_red['betr'].values[outliers_r], 'ok',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"outlier ({})\".format(outliers_r.sum()))\n",
    "ax3.plot(chsq_red['chisq'].values[bad_r], chsq_red['betr'].values[bad_r], 'sk',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"bad trace ({})\".format(bad_r.sum()))\n",
    "ax3.set_xlabel(r\"$\\chi^2$\")\n",
    "ax3.set_ylabel(r\"$\\beta$\")\n",
    "ax3.set_yscale('log')\n",
    "ax3.legend(**legend_opts)\n",
    "\n",
    "# GFP: km\n",
    "ax2.plot(chsq_green['chisq'].values[cluster_g], chsq_green['kmg'].values[cluster_g], '.g',\n",
    "         ms=2, label=\"accepted ({})\".format(cluster_g.sum()))\n",
    "ax2.plot(chsq_green['chisq'].values[outliers_g], chsq_green['kmg'].values[outliers_g], 'ok',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"outlier ({})\".format(outliers_g.sum()))\n",
    "ax2.plot(chsq_green['chisq'].values[bad_g], chsq_green['kmg'].values[bad_g], 'sk',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"bad trace ({})\".format(bad_g.sum()))\n",
    "ax2.set_ylabel(r\"$k_\\mathrm{m}$\")\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title(\"GFP\")\n",
    "ax2.legend(**legend_opts)\n",
    "\n",
    "# GFP: beta\n",
    "ax4.plot(chsq_green['chisq'].values[cluster_g], chsq_green['betg'].values[cluster_g], '.g',\n",
    "         ms=2, label=\"accepted ({})\".format(cluster_g.sum()))\n",
    "ax4.plot(chsq_green['chisq'].values[outliers_g], chsq_green['betg'].values[outliers_g], 'ok',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"outlier ({})\".format(outliers_g.sum()))\n",
    "ax4.plot(chsq_green['chisq'].values[bad_g], chsq_green['betg'].values[bad_g], 'sk',\n",
    "         ms=2, mew=0.5, mfc='none', label=\"bad trace ({})\".format(bad_g.sum()))\n",
    "ax4.set_xlabel(r\"$\\chi^2$\")\n",
    "ax4.set_ylabel(r\"$\\beta$\")\n",
    "ax4.set_yscale('log')\n",
    "ax4.legend(**legend_opts)\n",
    "\n",
    "f.tight_layout(pad=0, w_pad=2)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_chisquare_correlations_cluster.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control: Plot chi-square histograms for clustered data\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "red_vals = np.log10(chsq_red['chisq'].values)\n",
    "(_, _, patches) = ax1.hist((red_vals[cluster_r], red_vals[~cluster_r]), bins=50, stacked=True)\n",
    "plt.setp(patches[0], facecolor='r', edgecolor='r', lw=.5)\n",
    "plt.setp(patches[1], facecolor='none', edgecolor='k', lw=.5)\n",
    "ax1.set_title(r\"$\\chi^2$ for RFP\")\n",
    "ax1.set_xlabel(r\"$\\log(\\chi^2)$ [a.u.]\")\n",
    "ax1.set_ylabel(\"Occurrences\")\n",
    "\n",
    "green_vals = np.log10(chsq_green['chisq'].values)\n",
    "(_, _, patches) = ax2.hist((green_vals[cluster_g], green_vals[~cluster_g]), bins=50, stacked=True)\n",
    "plt.setp(patches[0], facecolor='g', edgecolor='g', lw=.5)\n",
    "plt.setp(patches[1], facecolor='none', edgecolor='k', lw=.5)\n",
    "ax2.set_title(r\"$\\chi^2$ for GFP\")\n",
    "ax2.set_xlabel(r\"$\\log(\\chi^2)$ [a.u.]\")\n",
    "ax2.set_ylabel(\"Occurrences\")\n",
    "\n",
    "f.tight_layout(pad=0)\n",
    "plt.show(f)\n",
    "with PdfPages(getOutpath(\"fixed_chisquare_histograms_cluster.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write complete single-cell results to XLSX file\n",
    "\n",
    "# Append “bad trace” and “outlier” to parameter table\n",
    "all_red = chsq_red\n",
    "all_red['bad'] = bad_traces_red\n",
    "all_red['outlier'] = outliers_r\n",
    "\n",
    "all_green = chsq_green\n",
    "all_green['bad'] = bad_traces_green\n",
    "all_green['outlier'] = outliers_g\n",
    "\n",
    "# Create dataset information table\n",
    "ds_info_tab = pd.DataFrame(index=[], columns=['Dataset', 'sample', 'condition', 'measurement'])\n",
    "for i, d in enumerate(D):\n",
    "    ds_info_tab.loc[i,'Dataset'] = i\n",
    "    for k in ('sample', 'condition', 'measurement'):\n",
    "        ds_info_tab.loc[i,k] = d[k]\n",
    "\n",
    "# Save complete single-cell results to spreadsheet\n",
    "xlsx_file = getOutpath(\"fixed_parameters_filter.xlsx\")\n",
    "xlsx_writer = pd.ExcelWriter(xlsx_file, engine='xlsxwriter')\n",
    "all_red.to_excel(xlsx_writer, sheet_name=\"red\", na_rep=\"NaN\")\n",
    "all_green.to_excel(xlsx_writer, sheet_name=\"green\", na_rep=\"NaN\")\n",
    "ds_info_tab.to_excel(xlsx_writer, sheet_name=\"info\")\n",
    "xlsx_writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "Based on the fits, the underlying parameter distributions are acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "This section contains code that was/is used for developing ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rug_plot(ax, data, y_min=0, y_max=0.1, **kwargs):\n",
    "    \"\"\"\n",
    "    Creates a rug plot in the given axis of the given data.\n",
    "\n",
    "    Input:\n",
    "        ax -- axes object in which to plot the rug plot\n",
    "        data -- numpy array of horizontal positions at which to plot the rug plot\n",
    "        y_min -- lower end of the rug plot in percent of axes (default: 0)\n",
    "        y_max -- upper end of the rug plot in percent of axes (default: 0.1)\n",
    "        kwargs -- (optional) keyword arguments to be passed to `axvline`\n",
    "\n",
    "    Returns:\n",
    "        list of return values of `axvline`\n",
    "\n",
    "    The rug plot is plotted as a loop of calls to `axvline` over the elements\n",
    "    of a flattened copy of the array `data`.\n",
    "    \"\"\"\n",
    "    return [ax.axvline(d, ymin=y_min, ymax=y_max, **kwargs) for d in data.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask how to set the x-axis limits\n",
    "restrict_xlim_to_data = False\n",
    "def toggle_xlim_restr(b):\n",
    "    global restrict_xlim_to_data\n",
    "    restrict_xlim_to_data = b['new']\n",
    "chbt = wdg.Checkbox(description=\"Restrict x-limits to clean data\")\n",
    "chbt.observe(toggle_xlim_restr, names=\"value\")\n",
    "IPython.display.display(chbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge all red datasets\n",
    "mrg_cnd = []\n",
    "mrg_idx = []\n",
    "for i, r in enumerate(R):\n",
    "    if 'rfp' not in r:\n",
    "        continue\n",
    "\n",
    "    mrg_cnd.append(r['rfp']['params'])\n",
    "    mrg_idx.append(i)\n",
    "    #print(\"{}: {}\".format(i, str(r['rfp']['params'].shape)))\n",
    "\n",
    "big_red = pd.concat(mrg_cnd, keys=mrg_idx)\n",
    "\n",
    "# Plot combined red parameter distributions\n",
    "pn_red = ('G0r', 'Gu0r', 'betr', 'kmr')\n",
    "kde = parameter_KDE(big_red,\n",
    "                    bw_div=(15, 15, 15, 500),\n",
    "                    dens_res=(200, 200, 200, 5000),\n",
    "                    nice_ends=False)\n",
    "\n",
    "# Prepare table of red parameter distribution moments\n",
    "red_dist_tab = pd.DataFrame(index=pn_red,\n",
    "                            columns=('mean', 'dirty_mean', 'std_dev',\n",
    "                                     'stat_total', 'stat_thresh'),\n",
    "                            dtype=np.float_)\n",
    "\n",
    "f, axa = plt.subplots(len(kde), 1, figsize=(8,12))\n",
    "\n",
    "for i, p in enumerate(pn_red):\n",
    "    clr_face = '#ff000055'\n",
    "    #clr_edge = '#990000ff'\n",
    "    #plot_kde(axa[i], kde[p], p, clr_face)\n",
    "\n",
    "    # Load data\n",
    "    data = big_red.loc[:,p].values\n",
    "\n",
    "    prob = kde[p]['prob']\n",
    "    idx = prob >= .03 * prob.max()\n",
    "    prob = prob[idx]\n",
    "    val = kde[p]['val'][idx]\n",
    "\n",
    "    max_val = val[-1]\n",
    "    idx_fit = np.logical_and(data <= max_val, data != 0)\n",
    "\n",
    "    mn = np.mean(data[idx_fit])\n",
    "    dirty_mean = np.mean(data)\n",
    "    std = np.std(data[idx_fit])\n",
    "    scale = np.abs(val[prob.argmax()] - val[np.abs(prob - prob.max() * .5).argmin()])\n",
    "    red_dist_tab.loc[p,:] = (mn, dirty_mean, std, data.size, idx_fit.sum())\n",
    "    #print(\"{:4s}: mean={:9.4f}, std={:9.4f}, scale={:9.4f}, dirty_mean={:9.4}\".format(\n",
    "    #    p, mn, std, scale, dirty_mean))\n",
    "\n",
    "    rug_plot(axa[i], data, color='#bbbbbb', lw=1)[0].set_label(\"Single observations\")\n",
    "    axa[i].plot(val, prob, '-k', label=\"Kernel density\".format(p))\n",
    "    axa[i].axvline(mn, color='b', label=\"Mean (clean)\")\n",
    "    axa[i].axvline(dirty_mean, color='r', ls='--', label=\"Mean (dirty)\")\n",
    "\n",
    "    # Format axes\n",
    "    y_intervall = np.array((0, min(prob.max() * 1.1, axa[i].get_ylim()[1])))\n",
    "    axa[i].set_ylim(y_intervall)\n",
    "\n",
    "    x_intervall = np.array((0, val.max()))\n",
    "    if not restrict_xlim_to_data:\n",
    "        x_intervall[1] = max(x_intervall[1], data[data <= 1.5 * val.max()].max())\n",
    "    x_intervall += np.array((-1, 1)) * .025 * (x_intervall[1] - x_intervall[0])\n",
    "    axa[i].set_xlim(x_intervall)\n",
    "\n",
    "    axa[i].text(x_intervall.sum() / 2, y_intervall[1] * .95, p,\n",
    "           verticalalignment='top', horizontalalignment='center', size='x-large',\n",
    "           bbox={'facecolor': 'red', 'edgecolor': 'none', 'alpha': .6})\n",
    "\n",
    "    axa[i].legend()\n",
    "\n",
    "axa[-1].set_xlabel(\"Parameter value [a.u.]\")\n",
    "axa[len(axa) // 2].set_ylabel(\"Frequency of occurrences\")\n",
    "f.tight_layout(pad=0)\n",
    "plt.show(f)\n",
    "\n",
    "with PdfPages(getOutpath(\"fixed_parameters_red.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above for all green parameters\n",
    "# Merge all green datasets\n",
    "mrg_cnd = []\n",
    "mrg_idx = []\n",
    "for i, r in enumerate(R):\n",
    "    if 'gfp' not in r:\n",
    "        continue\n",
    "\n",
    "    mrg_cnd.append(r['gfp']['params'])\n",
    "    mrg_idx.append(i)\n",
    "    #print(\"{}: {}\".format(i, str(r['rfp']['params'].shape)))\n",
    "\n",
    "big_green = pd.concat(mrg_cnd, keys=mrg_idx)\n",
    "\n",
    "# Plot combined red parameter distributions\n",
    "pn_green = ('G0g', 'Gu0g', 'betg', 'kmg')\n",
    "kde = parameter_KDE(big_green,\n",
    "                    bw_div=(20, 5000, 30, 15),\n",
    "                    dens_res=(250, 50000, 400, 200),\n",
    "                    nice_ends=False)\n",
    "\n",
    "# Prepare table of green parameter distribution moments\n",
    "green_dist_tab = pd.DataFrame(index=pn_green,\n",
    "                              columns=('mean', 'dirty_mean', 'std_dev',\n",
    "                                       'stat_total', 'stat_thresh'),\n",
    "                              dtype=np.float_)\n",
    "\n",
    "f, axa = plt.subplots(len(kde), 1, figsize=(8,12))\n",
    "\n",
    "for i, p in enumerate(pn_green):\n",
    "    clr_face = '#00ff0055'\n",
    "\n",
    "    # Load data\n",
    "    data = big_green.loc[:,p].values\n",
    "\n",
    "    prob = kde[p]['prob']\n",
    "    idx = prob >= .03 * prob.max()\n",
    "    prob = prob[idx]\n",
    "    val = kde[p]['val'][idx]\n",
    "\n",
    "    max_val = val[-1]\n",
    "    idx_fit = np.logical_and(data <= max_val, data != 0)\n",
    "\n",
    "    mn = np.mean(data[idx_fit])\n",
    "    dirty_mean = np.mean(data)\n",
    "    std = np.std(data[idx_fit])\n",
    "    scale = np.abs(val[prob.argmax()] - val[np.abs(prob - prob.max() * .5).argmin()])\n",
    "    green_dist_tab.loc[p,:] = (mn, dirty_mean, std, data.size, idx_fit.sum())\n",
    "    #print(\"{:4s}: mean={:9.4f}, std={:9.4f}, scale={:9.4f}, dirty_mean={:9.4}\".format(\n",
    "    #    p, mn, std, scale, dirty_mean))\n",
    "\n",
    "    # Plot dataset density\n",
    "    rug_plot(axa[i], data, color='#bbbbbb', lw=1)[0].set_label(\"Single observations\")\n",
    "    axa[i].plot(val, prob, '-k', label=\"Kernel density\".format(p))\n",
    "    axa[i].axvline(mn, color='b', label=\"Mean (clean)\")\n",
    "    axa[i].axvline(dirty_mean, color='r', ls='--', label=\"Mean (dirty)\")\n",
    "\n",
    "    # Format axes\n",
    "    y_intervall = np.array((0, min(prob.max() * 1.1, axa[i].get_ylim()[1])))\n",
    "    axa[i].set_ylim(y_intervall)\n",
    "\n",
    "    x_intervall = np.array((0, val.max()))\n",
    "    if not restrict_xlim_to_data:\n",
    "        x_intervall[1] = max(x_intervall[1], data[data <= 1.5 * val.max()].max())\n",
    "    x_intervall += np.array((-1, 1)) * .025 * (x_intervall[1] - x_intervall[0])\n",
    "    axa[i].set_xlim(x_intervall)\n",
    "\n",
    "    axa[i].text(x_intervall.sum() / 2, y_intervall[1] * .95, p,\n",
    "           verticalalignment='top', horizontalalignment='center', size='x-large',\n",
    "           bbox={'facecolor': 'green', 'edgecolor': 'none', 'alpha': .6})\n",
    "\n",
    "    axa[i].legend()\n",
    "\n",
    "axa[-1].set_xlabel(\"Parameter value [a.u.]\")\n",
    "axa[len(axa) // 2].set_ylabel(\"Frequency\")\n",
    "f.tight_layout(pad=0)\n",
    "plt.show(f)\n",
    "\n",
    "with PdfPages(getOutpath(\"fixed_parameters_green.pdf\")) as pdf:\n",
    "    pdf.savefig(f)\n",
    "\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "red_dist_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_dist_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save distribution parameters to spreadsheet\n",
    "xlsx_file = getOutpath(\"fixed_distribution_moments.xlsx\")\n",
    "xlsx_writer = pd.ExcelWriter(xlsx_file, engine='xlsxwriter')\n",
    "red_dist_tab.to_excel(xlsx_writer, sheet_name=\"red\")\n",
    "green_dist_tab.to_excel(xlsx_writer, sheet_name=\"green\")\n",
    "xlsx_writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print(big_red.shape)\n",
    "print(big_green.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [currently broken]\n",
    "ds = 0\n",
    "cond = D[ds]['condition']\n",
    "if cond == 'gfp':\n",
    "    ftprm = green_p\n",
    "    clr = 'g'\n",
    "elif cond == 'rfp':\n",
    "    ftprm = red_p\n",
    "    clr = 'r'\n",
    "params = ftprm.names()\n",
    "\n",
    "par_kde = parameter_KDE(R[ds][cond]['params'].loc[:,params])\n",
    "\n",
    "for pn, pp in par_kde.items():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(pp['val'], pp['prob'], '-', color=clr)\n",
    "    ax.set_title(pn)\n",
    "    ax.set_xlabel(\"Value [a.u.]\")\n",
    "    ax.set_ylabel(\"Occurrence frequency\")\n",
    "\n",
    "    #beta_par = sc.stats.beta.fit(\n",
    "    #    R[ds][cond]['params'].loc[:,pn],\n",
    "    #    loc=pp['val'][pp['prob'].argmax()],\n",
    "    #    scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1])\n",
    "    #ax.plot(pp['val'], sc.stats.beta.pdf(pp['val'], *beta_par), ':k')\n",
    "\n",
    "\n",
    "    gamma_par = sc.stats.gamma.fit(\n",
    "        R[ds][cond]['params'].loc[:,pn],\n",
    "        scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1],\n",
    "        floc=0)\n",
    "    ax.plot(pp['val'], sc.stats.gamma.pdf(pp['val'], *gamma_par), '--k')\n",
    "\n",
    "    lognorm_par = sc.stats.lognorm.fit(\n",
    "        R[ds][cond]['params'].loc[:,pn],\n",
    "        scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1],\n",
    "        floc=0)\n",
    "    ax.plot(pp['val'], sc.stats.lognorm.pdf(pp['val'], *lognorm_par), ':k')\n",
    "\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin distributions of the data sets (both separate and combined)\n",
    "pn_both = ()\n",
    "pn_red = ('Gu0r', 'betr', 'kmr')\n",
    "pn_green = ('Gu0g', 'betg', 'kmg')\n",
    "\n",
    "\n",
    "with PdfPages(getOutpath('parameter_distributions.pdf')) as pdf:\n",
    "    for ds in range(len(D)):\n",
    "        par_kde = {}\n",
    "        fit_types = []\n",
    "\n",
    "        # Check for separate and single fit\n",
    "        hasSeparate = False\n",
    "        hasSingle = False\n",
    "        if 'rfp' in R[ds] and 'gfp' in R[ds]:\n",
    "            hasSeparate = True\n",
    "            fit_types += ['rfp', 'gfp']\n",
    "        elif 'rfp' in R[ds]:\n",
    "            hasSingle = True\n",
    "            fit_types += ['rfp']\n",
    "        elif 'gfp' in R[ds]:\n",
    "            hasSingle = True\n",
    "            fit_types += ['gfp']\n",
    "\n",
    "        # Check for combined fit\n",
    "        par_kde_combined = {}\n",
    "        if 'combined' in R[ds]:\n",
    "            hasCombined = True\n",
    "            fit_types += ['combined']\n",
    "        else:\n",
    "            hasCombined = False\n",
    "\n",
    "        # Calculate parameter distributions\n",
    "        for t in fit_types:\n",
    "            par_kde[t] = parameter_KDE(R[ds][t]['params'])\n",
    "\n",
    "        # Plot parameter distributions\n",
    "        for typeName, hasType in zip(('separate', 'single', 'combined'),\n",
    "                                     (hasSeparate, hasSingle, hasCombined)):\n",
    "            if not hasType:\n",
    "                continue\n",
    "\n",
    "            if typeName == 'single':\n",
    "                if 'rfp' in fit_types:\n",
    "                    len_pn_type = len(pn_red)\n",
    "                elif 'gfp' in fit_types:\n",
    "                    len_pn_type = len(pn_green)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown fit types: {}\".format(fit_type))\n",
    "\n",
    "                grid = (1, len(pn_both) + len_pn_type)\n",
    "            else:\n",
    "                grid = (2, len(pn_both) + max(len(pn_red), len(pn_green)))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            gs = GridSpec(grid[0], grid[1])\n",
    "\n",
    "            if typeName == 'combined':\n",
    "                # Combined fit; define specific settings\n",
    "                pn_green_temp = pn_green\n",
    "                pn_red_temp = pn_red\n",
    "                offset_both = len(pn_both)\n",
    "                kde_label_green = 'combined'\n",
    "                kde_label_red = 'combined'\n",
    "\n",
    "                # Plot combined parameters\n",
    "                for pi, label in enumerate(pn_both):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((pi, 0), rowspan=2))\n",
    "                    data = par_kde['combined'][label]\n",
    "                    clr_face = '#0000ff55'\n",
    "                    #clr_edge = '#000099ff'\n",
    "                    plot_kde(ax, data, label, clr_face)\n",
    "            else:\n",
    "                # Separate fit; define specific settings\n",
    "                pn_green_temp = pn_both + pn_green\n",
    "                pn_red_temp = pn_both + pn_red\n",
    "                offset_both = 0\n",
    "                kde_label_green = 'gfp'\n",
    "                kde_label_red = 'rfp'\n",
    "\n",
    "            # Plot green parameters\n",
    "            if (typeName != 'single') or 'gfp' in fit_types:\n",
    "                for pi, par_label in enumerate(pn_green_temp):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((0, pi+offset_both)))\n",
    "                    data = par_kde[kde_label_green][par_label]\n",
    "                    clr_face = '#00ff0055'\n",
    "                    #clr_edge = '#009900ff'\n",
    "                    plot_kde(ax, data, par_label, clr_face)\n",
    "\n",
    "            # Plot red parameters\n",
    "            if (typeName != 'single') or 'rfp' in fit_types:\n",
    "                i_row = 0 if typeName == 'single' else 1\n",
    "                for pi, par_label in enumerate(pn_red_temp):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((i_row, pi+offset_both)))\n",
    "                    data = par_kde[kde_label_red][par_label]\n",
    "                    clr_face = '#ff000055'\n",
    "                    #clr_edge = '#990000ff'\n",
    "                    plot_kde(ax, data, par_label, clr_face)\n",
    "\n",
    "            # Show and close figure\n",
    "            fig.suptitle(getDataLabel(ds) + \" (\" + typeName + \" fit)\")\n",
    "            fig.tight_layout(pad=0, rect=(0, 0, 1, .93))\n",
    "            pdf.savefig(fig, bbox_inches='tight')\n",
    "            plt.show(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot parameter correlations\n",
    "\n",
    "# Get parameters to be correlated\n",
    "par_cor = (('tr', 'tg'), ('m_ktl', 'm_ktl'), ('kmr', 'kmg'),\n",
    "           ('betr', 'betg'), ('deltr', 'deltg'), ('offr', 'offg'))\n",
    "\n",
    "for i, r in enumerate(R):\n",
    "    with PdfPages(os.path.join(getOutpath(), '{:s}_parameter_correlations.pdf'.format(getTimeStamp()))) as pdf:\n",
    "        for pr, pg in par_cor:\n",
    "            # Get parameter values\n",
    "            valr = r['red']['params'].loc[:,pr].values\n",
    "            valg = r['green']['params'].loc[:,pg].values\n",
    "\n",
    "            # Sort out outliers\n",
    "            idx = np.ones(np.size(valr), dtype=np.bool_)\n",
    "            isr = valr.argsort()[-2:]\n",
    "            isg = valg.argsort()[-2:]\n",
    "\n",
    "            if valr[isr[0]] < 0.9 * valr[isr[1]]:\n",
    "                idx[isr[1]] = False\n",
    "            if valg[isg[0]] < 0.9 * valg[isg[1]]:\n",
    "                idx[isg[1]] = False\n",
    "\n",
    "            # Plot\n",
    "            fig = plt.figure(figsize=(4.5,4))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "            ax.plot(valr[idx], valg[idx], '.')\n",
    "\n",
    "            ax.set_autoscale_on(False)\n",
    "            lmt = np.array([ax.get_xlim(), ax.get_ylim()])\n",
    "            diag = (lmt[:,0].max(), lmt[:,1].min())\n",
    "            ax.plot(diag, diag, '-k')\n",
    "\n",
    "            ax.set_xlabel(pr, color='r')\n",
    "            ax.set_ylabel(pg, color='g')\n",
    "            ax.set_title(\"{}\\nCorrelation {} – {}\".format(getDataLabel(i), pr, pg))\n",
    "\n",
    "            fig.tight_layout(pad=0)\n",
    "            plt.show(fig)\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the parameter distributions for the datasets\n",
    "ds_keys = list(R.keys())\n",
    "ds_keys.sort()\n",
    "params = R[ds_keys[0]]['combined']['params'].columns\n",
    "grid = (len(params), len(ds_keys))\n",
    "i_col = 0\n",
    "\n",
    "pdffile = os.path.join(getOutpath(), '{:s}_parameters.pdf'.format(getTimeStamp()))\n",
    "with PdfPages(pdffile) as pdf:\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(grid[0] * .8 * fig.get_figheight())\n",
    "    fig.set_figwidth(grid[1] * .8 * fig.get_figwidth())\n",
    "\n",
    "    for ds in ds_keys:\n",
    "        i_row = 0\n",
    "        for p in params:\n",
    "            ax = plt.subplot2grid(grid, (i_row, i_col))\n",
    "            ax.hist(R[ds]['combined']['params'][p], bins=100)\n",
    "            if i_row == grid[0] - 1:\n",
    "                ax.set_xlabel('Value [a.u.]')\n",
    "            if i_col == 0:\n",
    "                ax.set_ylabel('Occurrences [#]')\n",
    "            ax.set_title('{:s}: {:s}'.format(ds, p))\n",
    "            i_row += 1\n",
    "        i_col += 1\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot onset time correlations\n",
    "pdffile = os.path.join(getOutpath(), '{:s}_onset_correlations.pdf'.format(getTimeStamp()))\n",
    "with PdfPages(pdffile) as pdf:\n",
    "    for k in R.keys():\n",
    "        fig = plt.figure()\n",
    "        plt.plot([0, 30], [0, 30], 'k-')\n",
    "        plt.plot(R[k]['combined']['params']['tr'], R[k]['combined']['params']['tg'], '.')\n",
    "        plt.xlabel('Onset RFP [h]')\n",
    "        plt.ylabel('Onset GFP [h]')\n",
    "        plt.title(k)\n",
    "        pdf.savefig(fig)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degradation rate ratio\n",
    "def plotHistograms(maxH):\n",
    "    Rkeys = sorted(R.keys())\n",
    "    for ds in Rkeys:\n",
    "        #deltg = R[ds]['green']['params']['deltg']\n",
    "        #deltr = R[ds]['red']['params']['deltr']\n",
    "        deltg = R[ds]['combined']['params']['deltg']\n",
    "        deltr = R[ds]['combined']['params']['deltr']\n",
    "        quot = deltg / deltr\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.hist(quot, bins=150, range=(0, maxH))\n",
    "        plt.title(ds)\n",
    "        plt.xlabel('$\\delta_\\mathrm{green} / \\delta_\\mathrm{red}$ [a.u.]')\n",
    "        plt.ylabel('Occurrences [#]')\n",
    "        plt.show(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "wdg.interact(plotHistograms, maxH=wdg.IntSlider(\n",
    "    value=100, min=0, max=1000, step=10, description='Histogram maximum', continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit distribution to degradation rate quotient histograms\n",
    "def gamma(x, p=2, b=1, s=10):\n",
    "    return s * b**p * x**(p-1) * np.exp(-b * x) / sc.special.gamma(p)\n",
    "\n",
    "def gamma2(x, p1=1.9, p2=2.1, b1=0.9, b2=1.1, s1=10, s2=10):\n",
    "    return gamma(x, p1, b1, s1) + gamma(x, p2, b2, s2)\n",
    "\n",
    "def weibull(x, lmbd=.2, k=2, s=10):\n",
    "    return s * lmbd * k * (lmbd * x)**(k - 1) * np.exp(- (lmbd * x)**k)\n",
    "\n",
    "def weibull2(x, lmbd1=.15, lmbd2=.25, k1=1.9, k2=2.1, s1=10, s2=10):\n",
    "    return weibull(x, lmbd=lmbd1, k=k1, s=s1) + weibull(x, lmbd=lmbd2, k=k2, s=s2)\n",
    "\n",
    "# Define models\n",
    "model_gamma = lm.Model(gamma)\n",
    "model_gamma.set_param_hint(name='p', min=.01)\n",
    "model_gamma.set_param_hint(name='b', min=.01)\n",
    "model_gamma.set_param_hint(name='s', min=1)\n",
    "\n",
    "model_gamma2 = lm.Model(gamma2)\n",
    "model_gamma2.set_param_hint(name='p1', min=.01)\n",
    "model_gamma2.set_param_hint(name='p2', min=.01)\n",
    "model_gamma2.set_param_hint(name='b1', min=.01)\n",
    "model_gamma2.set_param_hint(name='b2', min=.01)\n",
    "model_gamma2.set_param_hint(name='s1', min=1)\n",
    "model_gamma2.set_param_hint(name='s2', min=1)\n",
    "\n",
    "model_weibull = lm.Model(weibull)\n",
    "model_weibull.set_param_hint(name='lmbd', min=.001)\n",
    "model_weibull.set_param_hint(name='k', min=.001, max=5)\n",
    "model_weibull.set_param_hint(name='s', min=1)\n",
    "\n",
    "model_weibull2 = lm.Model(weibull2)\n",
    "model_weibull2.set_param_hint(name='lmbd1', min=.001)\n",
    "model_weibull2.set_param_hint(name='lmbd2', min=.001)\n",
    "model_weibull2.set_param_hint(name='k1', min=.001, max=5)\n",
    "model_weibull2.set_param_hint(name='k2', min=.001, max=5)\n",
    "model_weibull2.set_param_hint(name='s1', min=1)\n",
    "model_weibull2.set_param_hint(name='s2', min=1)\n",
    "\n",
    "maxH = 40\n",
    "\n",
    "with PdfPages(os.path.join(getOutpath(), '{:s}_degradation_distribution.pdf'.format(getTimeStamp()))) as pdf:\n",
    "    for ds in sorted(R.keys()):\n",
    "        # Calculate degradation rate quotient\n",
    "        deltg = R[ds]['combined']['params']['deltg']\n",
    "        deltr = R[ds]['combined']['params']['deltr']\n",
    "        quot = deltg / deltr\n",
    "\n",
    "        # Create histogram\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        hist_val, hist_edg = ax.hist(quot, bins=70, range=(0, maxH), label='Histogram')[:2]\n",
    "        hist_ctr = (hist_edg[:-1] + hist_edg[1:]) / 2\n",
    "\n",
    "        # Fit models\n",
    "        result_g = model_gamma.fit(hist_val, x=hist_ctr)\n",
    "        result_g2 = model_gamma2.fit(hist_val, x=hist_ctr)\n",
    "        result_w = model_weibull.fit(hist_val, x=hist_ctr)\n",
    "        result_w2 = model_weibull2.fit(hist_val, x=hist_ctr)\n",
    "\n",
    "        # Select models\n",
    "        #print('gamma: {}'.format(result_g.chisqr))\n",
    "        #print('gamma2: {}'.format(result_g2.chisqr))\n",
    "        #print('weibull: {}'.format(result_w.chisqr))\n",
    "        #print('weibull2: {}'.format(result_w2.chisqr))\n",
    "\n",
    "        if result_g2.chisqr < .7 * result_g.chisqr:\n",
    "            res_g = result_g2\n",
    "            name_g = 'gamma2'\n",
    "        else:\n",
    "            res_g = result_g\n",
    "            name_g = 'gamma'\n",
    "\n",
    "        if result_w2.chisqr < .7 * result_w.chisqr:\n",
    "            res_w = result_w2\n",
    "            name_w = 'weibull2'\n",
    "        else:\n",
    "            res_w = result_w\n",
    "            name_w = 'weibull'\n",
    "\n",
    "        # Plot models\n",
    "        x = np.linspace(.1, 5, 100)\n",
    "        ax.plot(hist_ctr, res_g.best_fit, '-', label=name_g, color='orange')\n",
    "        ax.plot(hist_ctr, res_w.best_fit, '-', label=name_w, color='magenta')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('$\\delta_\\mathrm{green} / \\delta_\\mathrm{red}$ [a.u.]')\n",
    "        ax.set_ylabel('Counts [#]')\n",
    "        ax.set_title(ds)\n",
    "\n",
    "        # Print fit reports\n",
    "        rep = res_g.fit_report(show_correl=False) + '\\n' + res_w.fit_report(show_correl=False)\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax.set_axis_off()\n",
    "        ax.text(0, 1, rep, ha='left', va='top', family='monospace', size=5.5)\n",
    "\n",
    "        # Display, save and close figure\n",
    "        plt.show(fig)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot of degradation rates\n",
    "Rkeys = sorted(R.keys())\n",
    "for ds in Rkeys:\n",
    "    deltg = R[ds]['combined']['params']['deltg']\n",
    "    deltr = R[ds]['combined']['params']['deltr']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    h = plt.plot(deltg, deltr, '.')\n",
    "    plt.title(ds)\n",
    "    plt.xlabel('$\\delta_\\mathrm{green}$ [a.u.]')\n",
    "    plt.ylabel('$\\delta_\\mathrm{red}$ [a.u.]')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
