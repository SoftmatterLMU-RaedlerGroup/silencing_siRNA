{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## siRNA knockdown fixed ##\n",
    "This notebook fits traces with fixed mRNA expression.\n",
    "\n",
    "An amount of mRNA is added to the cells.\n",
    "At a certain time $t=0$, a fixing agent (CHX) is added, and mRNA expression is stopped.\n",
    "Now, an initial amount $G_{u0}$ of pre-mature protein and an initial amount $G_0$ of mature protein is in the cell.\n",
    "\n",
    "After adding the fixing agent, the pre-mature protein can mature with maturation rate $k_m$, and both pre-mature and mature protein can degrade with degradation rate $\\beta$.\n",
    "\n",
    "This system can be described by the following differential equations:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\mathrm{d}G_u}{\\mathrm{d}t} &= -\\beta G_u - k_m G_u \\\\\n",
    "\\frac{\\mathrm{d}G}{\\mathrm{d}t} &= -\\beta G + k_m G_u\n",
    "\\end{align*}$$\n",
    "\n",
    "The solution for the amount of mature protein $G(t)$ is:\n",
    "$$\n",
    "G(t) = G_0 \\mathrm{e}^{-\\beta t} + G_{u0}\\left(\\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t}\\right)\n",
    "$$\n",
    "The parameters to fit are $\\beta$, $k_m$ and $G_{u0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook structure\n",
    "The notebook has the following structure:\n",
    "\n",
    "At first, the model functions are defined and the data is loaded. The next section contains code for fitting the two models separately. The next section contains code for fitting the two traces in one run with parameters shared among the models.\n",
    "\n",
    "Fitting requires that the result list `R` is defined, which can be done by running the corresponding cell. When `R` has been populated by fitting, the results can be plotted. There are cells for plotting the results of the separate fit, the results of the combined fit, and the pure parameter distributions of all fits.\n",
    "\n",
    "Additionally, there are cells for saving and loading paramaters by python’s `pickle` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules needed\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.seterr(divide='print')\n",
    "import scipy as sc\n",
    "#import lmfit as lm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import pickle\n",
    "import ipywidgets as wdg\n",
    "import IPython\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define utility functions\n",
    "def getTimeStamp():\n",
    "    \"\"\"Returns a human-readable string representation of the current time\"\"\"\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime(\"%Y-%m-%d–%H%M%S\")\n",
    "\n",
    "\n",
    "def getOutpath(filename='', timestamp=None):\n",
    "    \"\"\"Returns (and creates, if necessary) the path to a directory\n",
    "    called “out” inside the current directory.\n",
    "    If `filename` is given, the filename is appendet to the output directory.\n",
    "    A timestamp will be added to the filename if `timestamp != ''`.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    outpath = os.path.join(os.getcwd(), 'out')\n",
    "    if not os.path.isdir(outpath) and not os.path.lexists(outpath):\n",
    "        os.path.mkdir(outpath)\n",
    "\n",
    "    # If requested, build filename\n",
    "    if len(filename) > 0:\n",
    "        if timestamp == None:\n",
    "            timestamp = getTimeStamp()\n",
    "        outpath = os.path.join(outpath, ((timestamp + '_') if len(timestamp) > 0 else '') + filename)\n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(t, G0, Gu0, beta, km):\n",
    "    \"\"\"General fixed expression model function\"\"\"\n",
    "    return G0 * np.exp(-beta * t) + Gu0 * (np.exp(-beta * t) - np.exp(-(beta+km) * t))\n",
    "\n",
    "def red(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Model function for fixed RFP data\"\"\"\n",
    "    return model(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Model function for fixed GFP data\"\"\"\n",
    "    return model(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set default parameter values\n",
    "G0r_0 = 100\n",
    "Gu0r_0 = 1000\n",
    "betr_0 = 0.04\n",
    "kmr_0 = 0.3\n",
    "\n",
    "G0g_0 = 2000\n",
    "Gu0g_0 = 2000\n",
    "betg_0 = 0.04\n",
    "kmg_0 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FitParameters:\n",
    "    \"\"\"FitParameters facilitates managing values and bounds of fit parameters\"\"\"\n",
    "    def __init__(self, fun, independent=[], fixed=[]):\n",
    "        # Store function\n",
    "        self.fun = fun\n",
    "\n",
    "        # Get parameters of fun\n",
    "        params = inspect.signature(self.fun).parameters\n",
    "\n",
    "        # Build data frame of parameters\n",
    "        self.df = pd.DataFrame(columns=['value', 'min', 'max'],\n",
    "                               index=[p for p in params.keys()],\n",
    "                               dtype=np.float64)\n",
    "\n",
    "        # Set “independent” and “fixed” flag\n",
    "        self.df.add(pd.DataFrame(columns=['independent', 'fixed'], dtype=np.bool))\n",
    "        for p in self.df.index.values:\n",
    "            self.df.loc[p, 'independent'] = p in independent\n",
    "            self.df.loc[p, 'fixed'] = p in fixed\n",
    "\n",
    "        # Set default parameters\n",
    "        for p in self.df.index.values:\n",
    "            if params[p].default == inspect.Parameter.empty:\n",
    "                if self.df.loc[p, 'independent']:\n",
    "                    self.df.loc[p, 'value'] = np.NaN\n",
    "                else:\n",
    "                    self.df.loc[p, 'value'] = 0\n",
    "            else:\n",
    "                self.df.loc[p, 'value'] = params[p].default\n",
    "\n",
    "    def set(self, p, **props):\n",
    "        \"\"\"Allows user to change parameter properties\"\"\"\n",
    "        if p not in self.df.index.values:\n",
    "            raise KeyError(\"Unknown parameter name: {}\".format(par))\n",
    "\n",
    "        for prop, val in props.items():\n",
    "            if prop == 'value':\n",
    "                self.df.loc[p, 'value'] = val\n",
    "            elif prop == 'min':\n",
    "                self.df.loc[p, 'min'] = val\n",
    "            elif prop == 'max':\n",
    "                self.df.loc[p, 'max'] = val\n",
    "            elif prop == 'independent':\n",
    "                self.df.loc[p, 'independent'] = val\n",
    "            elif prop == 'fixed':\n",
    "                self.df.loc[p, 'fixed'] = val\n",
    "            else:\n",
    "                raise KeyError(\"Illegal parameter property: {}\".format(prop))\n",
    "\n",
    "    def eval_params(self, params=[], independent=True, **vals):\n",
    "        \"\"\"Returns parameters for evaluating the function.\n",
    "\n",
    "        Arguments:\n",
    "        params: optional list of values of free parameters\n",
    "        independent: optional switch whether independent variables are requested\n",
    "        vals: dictionary of parameter values\n",
    "\n",
    "        If a value for a parameter is specified in both `params` and `vals`,\n",
    "        the value from `vals` is used.\n",
    "        Values for independent parameters must be specified in `vals`.\n",
    "        If `independent == False`, the independent variable needn’t be specified\n",
    "        and will not be returned.\"\"\"\n",
    "        # Add additional values from `params` to vals\n",
    "        if len(params) != 0:\n",
    "            par_names = self.names()\n",
    "            if np.size(par_names) != len(params):\n",
    "                raise ValueError(\"Wrong number of parameters given ({})\".format(len(params)))\n",
    "            for pn, pv in zip(par_names, params):\n",
    "                if pn not in vals:\n",
    "                    vals[pn] = pv\n",
    "\n",
    "        # Fill values unspecified so far from `self.df`\n",
    "        for p in self.df.index.values:\n",
    "            if p not in vals:\n",
    "                if independent and self.df.loc[p, 'independent']:\n",
    "                    raise ValueError(\"Independent parameter `{}` not specified\".format(p))\n",
    "                else:\n",
    "                    vals[p] = self.df.loc[p, 'value']\n",
    "            elif not independent and self.df.loc[p, 'independent']:\n",
    "                del vals[p]\n",
    "        return vals\n",
    "\n",
    "    def eval(self, params=[], **vals):\n",
    "        \"\"\"Evaluates the function.\n",
    "\n",
    "        Arguments:\n",
    "        params: optional list of values of free parameters\n",
    "        vals: dictionary of parameter values\n",
    "\n",
    "        If a value for a parameter is specified in both `params` and `vals`,\n",
    "        the value from `vals` is used.\n",
    "        Values for independent parameters must be specified in `vals`.\"\"\"\n",
    "        return self.fun(**self.eval_params(params, **vals))\n",
    "\n",
    "    def freeIdx(self):\n",
    "        \"\"\"Returns a list of names of free parameters\"\"\"\n",
    "        return [p for p in self.df.index.values\n",
    "                if not (self.df.loc[p, 'independent'] or self.df.loc[p, 'fixed'])]\n",
    "\n",
    "    def bounds(self):\n",
    "        \"\"\"Returns a list of bound tuples of free parameters\n",
    "        for use in scipy.optimize.minimize\"\"\"\n",
    "        bnds = []\n",
    "        for p in self.freeIdx():\n",
    "            # Get parameter bounds\n",
    "            min_val = self.df.loc[p, 'min']\n",
    "            max_val = self.df.loc[p, 'max']\n",
    "\n",
    "            # Replace missing values with default minimum and maximum values\n",
    "            if np.isnan(min_val):\n",
    "                min_val = None\n",
    "            if np.isnan(max_val):\n",
    "                max_val = None\n",
    "\n",
    "            # Append to bounds list\n",
    "            bnds.append((min_val, max_val))\n",
    "        return bnds\n",
    "\n",
    "    def initial(self):\n",
    "        \"\"\"Returns a numpy.ndarray of initial values for use in scipy.optimize.minimize\"\"\"\n",
    "        return self.df.loc[self.freeIdx(), 'value'].values.copy()\n",
    "\n",
    "    def index(self, p):\n",
    "        \"\"\"Returns the index of a given parameter in the parameter vector\"\"\"\n",
    "        idx = np.flatnonzero(self.df.index.values == p)\n",
    "        if len(idx) == 0:\n",
    "            raise KeyError(\"Unknown parameter name: {}\".format(p))\n",
    "        return idx[0]\n",
    "\n",
    "    def names(self, onlyFree=True):\n",
    "        \"\"\"Returns an array of the parameter names.\n",
    "\n",
    "        If `onlyFree == True`, only free parameters are returned.\n",
    "        Else, all parameters (including independent and fixed parameters) are returned.\"\"\"\n",
    "        if onlyFree:\n",
    "            return np.array(self.freeIdx(), dtype=np.object_)\n",
    "        else:\n",
    "            return self.df.index.values.copy()\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"Returns a deep copy of this instance\"\"\"\n",
    "        return deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate models\n",
    "red_p = FitParameters(red, independent='t')\n",
    "red_p.set('G0r', min=0, value=G0r_0)\n",
    "red_p.set('Gu0r', min=0, value=Gu0r_0)\n",
    "red_p.set('betr', min=0, value=betr_0)\n",
    "red_p.set('kmr', min=0, value=kmr_0)\n",
    "\n",
    "green_p = FitParameters(green, independent='t')\n",
    "green_p.set('G0g', min=0, value=G0g_0)\n",
    "green_p.set('Gu0g', min=0, value=Gu0g_0)\n",
    "green_p.set('betg', min=0, value=betg_0)\n",
    "green_p.set('kmg', min=0, value=kmg_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "To increase the efficiency of fitting, the Jacobian matrix of the objective function is provided to the optimization routine.\n",
    "If the objective function is a typical negative log-likelihood function with normal distribution of residuals\n",
    "$$\n",
    " L(\\theta) = \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f(t\\mid\\theta)\\big)^2 \\text{,}\n",
    "$$\n",
    "where $D_t$ is the measured data at time $t$ and $f(t\\mid\\theta)$ is the value of the model function at time $t$ with parameters $\\theta$, the Jacobian is:\n",
    "$$\\begin{align}\n",
    "\\nabla L(\\theta) &= \\nabla \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\nabla \\frac{1}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{2}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\nabla\\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)\\big(\\nabla D_t - \\nabla  f\\left(t\\,\\middle|\\,\\theta\\right)\\big) \\\\\n",
    "&= -\\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big) \\nabla f\\left(t\\,\\middle|\\,\\theta\\right) \\\\\n",
    "\\end{align}$$\n",
    "We see that for calculating the Jacobian of the objective function we need the Jacobian of the model function.\n",
    "\n",
    "We use the general fixed expression model $G(t)$ function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian $\\nabla G\\left(t \\,\\middle|\\, G_0, G_{u0}, \\beta, k_m\\right)$ of the general fixed expression model function is the vector of the derivatives with respect to the various parameters:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial G}{\\partial G_0} &= \\mathrm{e}^{-\\beta t} \\\\\n",
    "\\frac{\\partial G}{\\partial G_{u0}} &= \\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\frac{\\partial G}{\\partial \\beta} &= -G_0 t \\mathrm{e}^{-\\beta t} - G_{u0} t \\left( \\mathrm{e}^{-(\\beta+k_m)t} \\right) = -t G(t)\\\\\n",
    "\\frac{\\partial G}{\\partial k_m} &= G_{u0} t \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_jacobian(t, G0, Gu0, beta, km):\n",
    "    \"\"\"Returns the Jacobi matrix of the general fixed expression model function\n",
    "    with time along axis=0 and parameters along axis=1\"\"\"\n",
    "\n",
    "    # Initialize Jacobian\n",
    "    jac = np.zeros((np.size(t), 3))\n",
    "\n",
    "    # Define abbreviations for frequent terms\n",
    "    ebt = np.exp(-beta * t)\n",
    "    ebkt = np.exp(-(beta + km) * t)\n",
    "\n",
    "    # Derive w.r.t. G0\n",
    "    #jac[:, 0] = ebt\n",
    "\n",
    "    # Derive w.r.t. Gu0\n",
    "    jac[:, 0] = ebt - ebkt\n",
    "\n",
    "    # Derive w.r.t. beta\n",
    "    jac[:, 1] = -t * (G0 * ebt + Gu0 * (ebt - ebkt))\n",
    "\n",
    "    # Derive w.r.t. km\n",
    "    jac[:, 2] = Gu0 * t * ebkt\n",
    "\n",
    "    return jac\n",
    "\n",
    "def red_jacobian(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Wrapper function for Jacobian of red model function\"\"\"\n",
    "    return general_jacobian(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green_jacobian(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Wrapper function for Jacobian of green model function\"\"\"\n",
    "    return general_jacobian(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian\n",
    "Analogously, the Hessian matrix is defined for better fit results:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial^2 L}{\\partial\\theta_2\\partial\\theta_1} &= \\frac{\\partial^2}{\\partial\\theta_2\\partial\\theta_1} \\sum_{t\\in T} \\frac{1}{2\\sigma_t^2} \\big(D_t - f\\left(t\\,\\middle|\\,\\theta\\right)\\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{\\partial^2}{\\partial\\theta_2\\partial\\theta_1} \\frac{1}{2\\sigma_t^2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big)^2 \\\\\n",
    "&= \\sum_{t\\in T} \\frac{2}{2\\sigma_t^2} \\frac{\\partial}{\\partial\\theta_2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\frac{\\partial}{\\partial\\theta_1} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2} \\frac{\\partial}{\\partial\\theta_2} \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\left(-\\frac{\\partial f(t)}{\\partial\\theta_1}\\right) \\\\\n",
    "&= \\sum_{t\\in T} \\frac{1}{\\sigma_t^2}\n",
    "\\left( \\frac{\\partial f(t)}{\\partial\\theta_2} \\frac{\\partial f(t)}{\\partial\\theta_1} - \\big( D_t - f\\left(t\\,\\middle|\\,\\theta\\right) \\big) \\frac{\\partial^2 f(t)}{\\partial\\theta_2\\partial\\theta_1} \\right) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "The second order derivatives of the general fixed expression model function are:\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0}^2} &= 0 \\\\\n",
    "\\frac{\\partial^2 G}{\\partial \\beta^2} &= G_0 t^2 \\mathrm{e}^{-\\beta t} + G_{u0} t^2 \\left( \\mathrm{e}^{-\\beta t} - \\mathrm{e}^{-(\\beta+k_m)t} \\right) \\\\\n",
    "\\frac{\\partial^2 G}{\\partial k_m^2} &= -G_{u0} t^2 \\mathrm{e}^{-(\\beta+k_m)t}\\\\\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0} \\partial \\beta} &= t \\left( \\mathrm{e}^{-(\\beta+k_m)t} - \\mathrm{e}^{-\\beta t} \\right) \\\\\n",
    "\\frac{\\partial^2 G}{\\partial G_{u0} \\partial k_m} &= t \\mathrm{e}^{-(\\beta+k_m)t} \\\\\n",
    "\\frac{\\partial^2 G}{\\partial \\beta \\partial k_m} &= -G_{u0} t^2 \\mathrm{e}^{-(\\beta+k_m)t}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def general_hessian(t, G0, Gu0, beta, km):\n",
    "    \"\"\"Returns the Hessian matrix of the general fixed expression model function\n",
    "    with time along axis=0 and parameters along axis=1\"\"\"\n",
    "\n",
    "    # Initialize Hessian\n",
    "    hes = np.zeros((np.size(t), 6))\n",
    "\n",
    "    # Define abbreviations for frequent terms\n",
    "    ebt = np.exp(-beta * t)\n",
    "    ebkt = np.exp(-(beta + km) * t)\n",
    "    t2 = t**2\n",
    "\n",
    "    # Derive w.r.t. Gu0\n",
    "    # hes[:, 0] equals 0, do nothing\n",
    "\n",
    "    # Derive w.r.t. beta\n",
    "    hes[:, 1] = G0 * t2 * ebt + Gu0 * t2 * (ebt - ebkt)\n",
    "\n",
    "    # Derive w.r.t. km\n",
    "    hes[:, 2] = -Gu0 * t2 * ebkt\n",
    "\n",
    "    # Derive w.r.t. Gu0 and beta\n",
    "    hes[:, 3] = t * (ebkt - ebt)\n",
    "\n",
    "    # Derive w.r.t. Gu0 and km\n",
    "    hes[:, 4] = t * ebkt\n",
    "\n",
    "    # Derive w.r.t. beta and km\n",
    "    hes[:, 5] = -Gu0 * t2 * ebkt\n",
    "\n",
    "    return hes\n",
    "\n",
    "def red_hessian(t, G0r, Gu0r, betr, kmr):\n",
    "    \"\"\"Wrapper function for Hessian of red model function\"\"\"\n",
    "    return general_hessian(t=t, G0=G0r, Gu0=Gu0r, beta=betr, km=kmr)\n",
    "\n",
    "def green_hessian(t, G0g, Gu0g, betg, kmg):\n",
    "    \"\"\"Wrapper function for Hessian of green model function\"\"\"\n",
    "    return general_hessian(t=t, G0=G0g, Gu0=Gu0g, beta=betg, km=kmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and prepare result list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate kernel density estimation of parameter distributions\n",
    "def parameter_KDE(par_tab, nice_ends=True):\n",
    "    \"\"\"Returns a kernel density estimation of the parameter values for plotting.\n",
    "    \n",
    "    Input parameters:\n",
    "        par_tab: pandas.DataFrame with parameters as columns and realizations as rows\n",
    "        nice_ends: (optional) if `True`, the start and end points will be set to 0\n",
    "\n",
    "    Returns:\n",
    "        dict with parameter names as keys. The values are dicts with keys \"val\" and \"prob\".\n",
    "        \"val\" contains an array of parameter values.\n",
    "        \"prob\" contains an array of the same length as for \"val\", containing the\n",
    "        probabilities/relative \n",
    "    \"\"\"\n",
    "    dens_res = 200\n",
    "    bw_div = 15\n",
    "\n",
    "    par_dist = {}\n",
    "\n",
    "    for par_name in par_tab.columns:\n",
    "        # Get parameter values\n",
    "        par_vals = par_tab.loc[:,par_name].values\n",
    "        par_vals = par_vals.reshape((par_vals.size, 1))\n",
    "\n",
    "        # Test parameter values for validity\n",
    "        if np.any(np.logical_not(np.isfinite(par_vals))):\n",
    "            print(\"Warning: invalid values encountered for “{}”\".format(par_name))\n",
    "            par_vals = par_vals(np.isfinite(par_vals))\n",
    "            if par_vals.size > 0:\n",
    "                # Calculate distribution of valid entries\n",
    "                par_vals = par_vals.reshape((par_vals.size, 1))\n",
    "            else:\n",
    "                # No valid entries found; cancel distribution calculation\n",
    "                par_dist[par_name] = {'val': [], 'prob': []}\n",
    "                continue\n",
    "\n",
    "        # Get parameter extrema and bandwidth\n",
    "        par_min = np.min(par_vals)\n",
    "        par_max = np.max(par_vals)\n",
    "        bw = (par_max - par_min) / bw_div\n",
    "\n",
    "        # Get kernel density estimation of parameter values\n",
    "        kde = KernelDensity(kernel='epanechnikov', bandwidth=bw).fit(par_vals)\n",
    "        par_x = np.linspace(par_min, par_max, dens_res).reshape((dens_res, 1))\n",
    "        par_dens = np.exp(kde.score_samples(par_x))\n",
    "\n",
    "        # Adjust values for nicer plotting (KDE >= 0, edges == 0)\n",
    "        #par_dens[par_dens < 0] = 0\n",
    "        if par_dens[0] != 0:\n",
    "            par_dens = np.insert(par_dens, 0, 0)\n",
    "            par_x = np.insert(par_x, 0, par_min)\n",
    "        if par_dens[-1] != 0:\n",
    "            par_dens = np.append(par_dens, 0)\n",
    "            par_x = np.append(par_x, par_max)\n",
    "\n",
    "        # Insert KDE into dict\n",
    "        par_dist[par_name] = {'val': par_x.flatten(), 'prob': par_dens.flatten()}\n",
    "    return par_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_kde(ax, dist, label, clr_face='b', clr_edge='k', mark=None):\n",
    "    \"\"\"Plots the current parameter value in relation to the distribution\n",
    "    in the whole dataset.\"\"\"\n",
    "    ax.fill_betweenx(dist['val'], dist['prob'], color=clr_face)\n",
    "    if mark != None:\n",
    "        ax.axhline(y=mark, color=clr_edge)\n",
    "    ax.set_xticks([])\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    for s in [ax.spines[pos] for pos in ['bottom', 'right', 'top']]:\n",
    "        s.set_visible(False)\n",
    "    ax.set_title(label)\n",
    "    #ax.get_yaxis().set_major_formatter(StrMethodFormatter('{x:.2g}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare data loading\n",
    "\n",
    "# Define available files\n",
    "datafiles = [\n",
    "    {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq6\",\n",
    "        \"file\": \"data/2017-09-08_seq6_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq7\",\n",
    "        \"file\": \"data/2017-09-08_seq7_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq8\",\n",
    "        \"file\": \"data/2017-09-08_seq8_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq9\",\n",
    "        \"file\": \"data/2017-09-08_seq9_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq10\",\n",
    "        \"file\": \"data/2017-09-08_seq10_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"rfp\",\n",
    "        \"measurement\": \"2017-09-08_seq11\",\n",
    "        \"file\": \"data/2017-09-08_seq11_Huh7_CayRFP_CHX_#molecules.xlsx\"\n",
    "    }, {\n",
    "        \"sample\": \"Huh7\",\n",
    "        \"condition\": \"gfp\",\n",
    "        \"measurement\": \"2017-08-18\",\n",
    "        \"file\": \"data/2017-08-18_Huh7_eGFP_CHX_#molecules.xlsx\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# By default, mark all files for loading\n",
    "load_idcs = range(len(datafiles))\n",
    "\n",
    "# Define function for loading data\n",
    "def load_data_from_files():\n",
    "    \"\"\"Loads data from specified files into `D`.\n",
    "    Requires `load_idcs` to hold a list of indices to `datafiles`.\"\"\"\n",
    "    global D\n",
    "    D = []\n",
    "    for i in load_idcs:\n",
    "        # Show message\n",
    "        print(\"Loading file: {}\".format(datafiles[i][\"file\"]))\n",
    "\n",
    "        # Read sheets from excel file\n",
    "        X = pd.read_excel(datafiles[i]['file'], dtype=np.float64, sheetname=[\n",
    "            '#RFP', '#GFP', '#RFP_error', '#GFP_error'])\n",
    "\n",
    "        # Write data into easy-to-access structure\n",
    "        d = {}\n",
    "        d['sample'] = datafiles[i]['sample']\n",
    "        d['condition'] = datafiles[i]['condition']\n",
    "        d['measurement'] = datafiles[i]['measurement']\n",
    "        d['file'] = datafiles[i]['file']\n",
    "\n",
    "        if d['condition'] == 'gfp':\n",
    "            d['t'] = X['#GFP'].values[:,0].flatten()\n",
    "        else:\n",
    "            d['t'] = X['#RFP'].values[:,0].flatten()\n",
    "        #d['rfp'] = X['RFP'].values[:,1:]\n",
    "        #d['gfp'] = X['GFP_corrected'].values[:,1:]\n",
    "        d['rfp'] = X['#RFP'].values[:,1:]\n",
    "        d['gfp'] = X['#GFP'].values[:,1:]\n",
    "        d['rfp_error'] = X['#RFP_error'].values[:,1:]\n",
    "        d['gfp_error'] = X['#GFP_error'].values[:,1:]\n",
    "        D.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDataLabel(i, filename=False):\n",
    "    \"\"\"Returns a nicely formatted name for the `i`-th element of `D`.\n",
    "    Set `filename=True` for a filename-friendly output.\"\"\"\n",
    "    if filename:\n",
    "        return \"{0[measurement]}_{0[sample]}_{0[condition]}\".format(D[i])\n",
    "    return \"{0[sample]}: {0[condition]} [{0[measurement]}]\".format(D[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in data from excel sheets\n",
    "\n",
    "# Prompt user for files to load\n",
    "lbl = wdg.Label('Select the files to load:')\n",
    "lbl.layout.width = 'initial'\n",
    "entries = []\n",
    "for f in datafiles:\n",
    "    entries.append(\"{} {}: {}\".format(\n",
    "        f['sample'], f['condition'], f['file']))\n",
    "sel_entry = wdg.SelectMultiple(options=entries, rows=len(entries))\n",
    "sel_entry.layout.width = 'initial'\n",
    "bload = wdg.Button(description='Load')\n",
    "bselall = wdg.Button(description='Select all')\n",
    "bselnone = wdg.Button(description='Select none')\n",
    "\n",
    "# Define callbacks\n",
    "def sel_all_files(_):\n",
    "    sel_entry.value = entries\n",
    "def sel_no_files(_):\n",
    "    sel_entry.value = ()\n",
    "def load_button_clicked(_):\n",
    "    global load_idcs\n",
    "    load_idcs = [entries.index(r) for r in sel_entry.value]\n",
    "    vb.close()\n",
    "    load_data_from_files()\n",
    "bselall.on_click(sel_all_files)\n",
    "bselnone.on_click(sel_no_files)\n",
    "bload.on_click(load_button_clicked)\n",
    "\n",
    "# Finally, show the widgets\n",
    "vb = wdg.VBox((lbl, sel_entry, wdg.HBox((bload,bselall,bselnone))))\n",
    "IPython.display.display(vb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Provide output tables\n",
    "\n",
    "# Initialize result dictionary\n",
    "R = []\n",
    "\n",
    "# Get a list of fit parameters\n",
    "#par_names = red_p.names().tolist()\n",
    "#par_names.extend(p for p in red_p.names() if p not in par_names)\n",
    "#par_names.sort()\n",
    "\n",
    "# Iteratively populate the result dictionary\n",
    "for k in range(len(D)):\n",
    "    R.insert(k, {})\n",
    "\n",
    "    cond = D[k]['condition']\n",
    "    if cond == 'rfp':\n",
    "        cols = red_p.names()\n",
    "    elif cond == 'gfp':\n",
    "        cols = green_p.names()\n",
    "\n",
    "    nTraces = np.shape(D[k][cond])[1]\n",
    "    nTimes = np.shape(D[k][cond])[0]\n",
    "    tpl_traces = np.empty((nTimes, nTraces))\n",
    "    tpl_traces.fill(np.NaN)\n",
    "\n",
    "    R[k][cond] = {}\n",
    "    R[k][cond]['params'] = pd.DataFrame(index=np.arange(nTraces), columns=cols, dtype='float64')\n",
    "    R[k][cond]['fit'] = np.copy(tpl_traces)\n",
    "    R[k][cond]['success'] = np.zeros(nTraces, dtype=np.bool_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle or load fitting results\n",
    "Pickling is only reasonable if the result list `R` has already been populated by fitting (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle fit results for future sessions\n",
    "outfile = getTimeStamp() + '_fit_results.pickled'\n",
    "with open(outfile, 'wb') as f:\n",
    "    pickle.dump(R, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pickled results (requires file suffix “.pickled”)\n",
    "pickfiles = [f for f in os.listdir() if f.lower().endswith('.pickled')]\n",
    "pickfiles.sort(reverse=True)\n",
    "\n",
    "lbl = wdg.Label('Select the file to load:')\n",
    "lbl.layout.width = 'initial'\n",
    "rad = wdg.RadioButtons(options=pickfiles)\n",
    "but = wdg.Button(description='Load')\n",
    "vb = wdg.VBox([lbl, rad, but])\n",
    "IPython.display.display(vb)\n",
    "\n",
    "def clicked_on_but(b):\n",
    "    global R\n",
    "    with open(rad.value, 'rb') as f:\n",
    "        R = pickle.load(f)\n",
    "    print('Loaded: ' + rad.value)\n",
    "    vb.close()\n",
    "but.on_click(clicked_on_but)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and plot separate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSeparate(ds, tr, pdf=None, par_kde=None):\n",
    "    \"\"\"Fits and plots the data, treating RFP and GFP separately.\n",
    "\n",
    "    Keyword arguments:\n",
    "    ds -- the dictionary key of the dataset\n",
    "    tr -- the index of the trace in the dataset to be processed\n",
    "    pdf -- a PdfPages object to which the figure is written if it is not None\n",
    "    par_kde -- if containing dict of values of parameter distributions, plot distributions\n",
    "    \"\"\"\n",
    "\n",
    "    # Get trace information\n",
    "    cond = D[ds]['condition']\n",
    "\n",
    "    # Plot fit results\n",
    "    fig = plt.figure()\n",
    "\n",
    "    if par_kde != None:\n",
    "        fig.set_figwidth(1.6 * fig.get_figwidth())\n",
    "\n",
    "        if cond == 'rfp':\n",
    "            pn = ['Gu0r', 'betr', 'kmr']\n",
    "            clr_face = '#ff000055'\n",
    "            clr_edge = '#990000ff'\n",
    "        elif cond == 'gfp':\n",
    "            pn = ['Gu0g', 'betg', 'kmg']\n",
    "            clr_face = '#00ff0055'\n",
    "            clr_edge = '#009900ff'\n",
    "\n",
    "        grid = (1, len(pn))\n",
    "        gs = GridSpec(grid[0], grid[1])\n",
    "\n",
    "        # Plot parameters\n",
    "        for pi, label in enumerate(pn):\n",
    "            ax = fig.add_subplot(gs[0, pi])\n",
    "            data = par_kde[label]\n",
    "            curr_val = R[ds][cond]['params'].loc[tr,label]\n",
    "            plot_kde(ax, data, label, clr_face, clr_edge, curr_val)\n",
    "\n",
    "        # Adjust subplot layout\n",
    "        gs.tight_layout(fig, pad=0, rect=(0.5, 0, 1, 1))\n",
    "\n",
    "        # Create axes for fit\n",
    "        gs_fit = GridSpec(1, 1)\n",
    "        ax = fig.add_subplot(gs_fit[0])\n",
    "        gs_fit.tight_layout(fig, pad=0, rect=(0, 0, 0.5, 1))\n",
    "\n",
    "    else:\n",
    "        ax = fig.gca()\n",
    "\n",
    "    if cond == 'rfp':\n",
    "        p_f, = ax.plot(D[ds]['t'], R[ds][cond]['fit'][:,tr], '-', label='RFP (fit)', color='#ff0000', linewidth=1)\n",
    "        p_d, = ax.plot(D[ds]['t'], D[ds][cond][:,tr], '-', label='RFP (measured)', color='#990000', linewidth=.5)\n",
    "    elif cond == 'gfp':\n",
    "        p_f, = ax.plot(D[ds]['t'], R[ds][cond]['fit'][:,tr], '-', label='GFP (fit)', color='#00ff00', linewidth=1)\n",
    "        p_d, = ax.plot(D[ds]['t'], D[ds][cond][:,tr], '-', label='GFP (measured)', color='#009900', linewidth=.5)\n",
    "\n",
    "    # Format plot\n",
    "    ax.set_xlabel('Time [h]')\n",
    "    ax.set_ylabel('Number of molecules [10³]')\n",
    "    ax.set_title('{} #{:03d}\\n(separate fit)'.format(getDataLabel(ds), tr))\n",
    "    ax.legend(handles=[p_d, p_f])\n",
    "\n",
    "    # Write figure to pdf\n",
    "    if pdf != None:\n",
    "        pdf.savefig(fig, bbox_inches='tight')\n",
    "\n",
    "    # Show and close figure\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit traces separately\n",
    "for ds in range(len(D)):\n",
    "\n",
    "    # Define data-dependent functions and parameters\n",
    "    cond = D[ds]['condition']\n",
    "    if cond == 'rfp':\n",
    "        ftprm = red_p\n",
    "        mdl_fcn = red\n",
    "        jac_fcn = red_jacobian\n",
    "        hes_fcn = red_hessian\n",
    "    elif cond == 'gfp':\n",
    "        ftprm = green_p\n",
    "        mdl_fcn = green\n",
    "        jac_fcn = green_jacobian\n",
    "        hes_fcn = green_hessian\n",
    "\n",
    "    nTraces = np.shape(D[ds][cond])[1]\n",
    "\n",
    "    for tr in range(nTraces):\n",
    "        print('Fitting „{}“ #{:03d}/{:03d} …'.format(getDataLabel(ds), tr, nTraces))\n",
    "        \n",
    "        # Prepare data\n",
    "        time = D[ds]['t']\n",
    "        data = D[ds][cond][:,tr].flatten()\n",
    "        wght = 1 / D[ds][cond + '_error'][:,tr]**2\n",
    "\n",
    "        # Prepare parameters\n",
    "        ftprm.set('G0' + cond[0], fixed=True, value=data[0])\n",
    "        ftprm.set('Gu0' + cond[0], value=.5*(data.max() - data.min()))\n",
    "\n",
    "        # Objective function (closure)\n",
    "        def objective_fcn(params):\n",
    "            \"\"\"Objective function for separate model\"\"\"\n",
    "            cur_val = ftprm.eval(params, t=time)\n",
    "            chisq = np.sum((data - cur_val)**2 * wght)\n",
    "            return chisq\n",
    "\n",
    "        # Jacobian/gradient (closure)\n",
    "        def gradient_fcn(params):\n",
    "            \"\"\"Gradient for separate model\"\"\"\n",
    "            J = jac_fcn(**ftprm.eval_params(params, t=time))\n",
    "            residuals = (data - ftprm.eval(params, t=time)).reshape((np.size(time),1))\n",
    "            vrnc = wght.reshape(np.shape(residuals))\n",
    "            return -np.sum(J * residuals * vrnc, axis=0).flatten()\n",
    "\n",
    "        # Hessian/second derivative (closure)\n",
    "        #def secderiv_fcn(params):\n",
    "        #    \"\"\"Hessian for separate model\"\"\"\n",
    "        #    H = hes_fcn(**ftprm.eval_params(params, t=time))\n",
    "        #    residuals = (data - ftprm.eval(params, t=time)).reshape((np.size(time), 1))\n",
    "        #    jac = jac_fcn(**ftprm.eval_params(params, t=time))\n",
    "        #    vrnc = wght.reshape((np.size(time), 1, 1))\n",
    "\n",
    "        #    H *= residuals\n",
    "        #    hes = np.empty((np.size(time), 3, 3))\n",
    "        #    for i in range(3):\n",
    "        #        hes[:,i,i] = jac[:,i]**2 - H[:,i]\n",
    "        #    hes[:,0,1] = jac[:,0] * jac[:,1] - H[:,3]\n",
    "        #    hes[:,0,2] = jac[:,0] * jac[:,2] - H[:,4]\n",
    "        #    hes[:,1,2] = jac[:,1] * jac[:,2] - H[:,5]\n",
    "        #    hes[:,1,0] = hes[:,0,1]\n",
    "        #    hes[:,2,0] = hes[:,0,2]\n",
    "        #    hes[:,2,1] = hes[:,1,2]\n",
    "        #    hes *= vrnc\n",
    "\n",
    "        #    return np.sum(hes, axis=0)\n",
    "\n",
    "        # Fit the data\n",
    "        result = sc.optimize.minimize(objective_fcn,\n",
    "                                      ftprm.initial(),\n",
    "                                      method='TNC',# one of: 'SLSQP' 'TNC' 'L-BFGS-B'\n",
    "                                      bounds=ftprm.bounds(),\n",
    "                                      jac=gradient_fcn,\n",
    "                                      #hess=secderiv_fcn,\n",
    "                                      options={'disp':True,\n",
    "                                               'maxiter': 20000}\n",
    "                                     )\n",
    "        #result = sc.optimize.least_squares(\n",
    "        #    objective_fcn,\n",
    "        #    ftprm.initial(),\n",
    "        #    jac=gradient_fcn,\n",
    "        #    bounds=(0, np.inf),\n",
    "        #    max_nfev=20000\n",
    "        #)\n",
    "\n",
    "        # Print result\n",
    "        print(\"\\tSuccess {}: {}\".format(result.success, result.message))\n",
    "\n",
    "        # Save results to R\n",
    "        R[ds][cond]['params'].iloc[tr] = ftprm.eval_params(result.x, independent=False)\n",
    "        best_fit = ftprm.eval(result.x, t=time)\n",
    "        #best_fit = mdl_fcn(time, *result.x)\n",
    "        R[ds][cond]['fit'][:,tr] = best_fit\n",
    "        R[ds][cond]['success'][tr] = result.success\n",
    "\n",
    "        # DEBUG\n",
    "        #if tr >= 2:\n",
    "        #    print(\"Breaking loop for debugging purposes\")\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot results of separate fit\n",
    "ts = getTimeStamp()\n",
    "\n",
    "for ds in range(len(D)):\n",
    "    cond = D[ds]['condition']\n",
    "    par_kde = parameter_KDE(R[ds][cond]['params'])\n",
    "    pdffile = getOutpath('separate_{}.pdf'.format(getDataLabel(ds, True)))\n",
    "    with PdfPages(pdffile) as pdf:\n",
    "        for tr in range(np.shape(D[ds][cond])[1]):\n",
    "            plotSeparate(ds, tr, pdf, par_kde)\n",
    "\n",
    "            # DEBUG\n",
    "            #if tr >= 2:\n",
    "            #    print(\"Break loop\")\n",
    "            #    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions\n",
    "Based on the fits, the underlying parameter distributions are acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = 0\n",
    "cond = D[ds]['condition']\n",
    "if cond == 'gfp':\n",
    "    ftprm = green_p\n",
    "    clr = 'g'\n",
    "elif cond == 'rfp':\n",
    "    ftprm = red_p\n",
    "    clr = 'r'\n",
    "params = ftprm.names()\n",
    "\n",
    "par_kde = parameter_KDE(R[ds][cond]['params'].loc[:,params])\n",
    "\n",
    "for pn, pp in par_kde.items():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.plot(pp['val'], pp['prob'], '-', color=clr)\n",
    "    ax.set_title(pn)\n",
    "    ax.set_xlabel(\"Value [a.u.]\")\n",
    "    ax.set_ylabel(\"Occurrence frequency\")\n",
    "\n",
    "    #beta_par = sc.stats.beta.fit(\n",
    "    #    R[ds][cond]['params'].loc[:,pn],\n",
    "    #    loc=pp['val'][pp['prob'].argmax()],\n",
    "    #    scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1])\n",
    "    #ax.plot(pp['val'], sc.stats.beta.pdf(pp['val'], *beta_par), ':k')\n",
    "\n",
    "\n",
    "    gamma_par = sc.stats.gamma.fit(\n",
    "        R[ds][cond]['params'].loc[:,pn],\n",
    "        scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1],\n",
    "        floc=0)\n",
    "    ax.plot(pp['val'], sc.stats.gamma.pdf(pp['val'], *gamma_par), '--k')\n",
    "\n",
    "    lognorm_par = sc.stats.lognorm.fit(\n",
    "        R[ds][cond]['params'].loc[:,pn],\n",
    "        scale=pp['val'][(pp['prob']>=0.3*pp['prob'].max())][-1],\n",
    "        floc=0)\n",
    "    ax.plot(pp['val'], sc.stats.lognorm.pdf(pp['val'], *lognorm_par), ':k')\n",
    "\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "This section contains code that was/is used for developing ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all red datasets\n",
    "mrg_cnd = []\n",
    "mrg_idx = []\n",
    "for i, r in enumerate(R):\n",
    "    if 'rfp' not in r:\n",
    "        continue\n",
    "\n",
    "    mrg_cnd.append(r['rfp']['params'])\n",
    "    mrg_idx.append(i)\n",
    "    print(\"{}: {}\".format(i, str(r['rfp']['params'].shape)))\n",
    "\n",
    "big_red = pd.concat(mrg_cnd, keys=mrg_idx)\n",
    "\n",
    "# Plot combined red parameter distributions\n",
    "pn_red = ('G0r', 'Gu0r', 'betr', 'kmr')\n",
    "kde = parameter_KDE(big_red)\n",
    "\n",
    "f, axa = plt.subplots(1, len(kde))\n",
    "\n",
    "for i, p in enumerate(pn_red):\n",
    "    clr_face = '#ff000055'\n",
    "    #clr_edge = '#990000ff'\n",
    "    plot_kde(axa[i], kde[p], p, clr_face)\n",
    "\n",
    "plt.show(f)\n",
    "plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin distributions of the data sets (both separate and combined)\n",
    "pn_both = ()\n",
    "pn_red = ('Gu0r', 'betr', 'kmr')\n",
    "pn_green = ('Gu0g', 'betg', 'kmg')\n",
    "\n",
    "\n",
    "with PdfPages(getOutpath('parameter_distributions.pdf')) as pdf:\n",
    "    for ds in range(len(D)):\n",
    "        par_kde = {}\n",
    "        fit_types = []\n",
    "\n",
    "        # Check for separate and single fit\n",
    "        hasSeparate = False\n",
    "        hasSingle = False\n",
    "        if 'rfp' in R[ds] and 'gfp' in R[ds]:\n",
    "            hasSeparate = True\n",
    "            fit_types += ['rfp', 'gfp']\n",
    "        elif 'rfp' in R[ds]:\n",
    "            hasSingle = True\n",
    "            fit_types += ['rfp']\n",
    "        elif 'gfp' in R[ds]:\n",
    "            hasSingle = True\n",
    "            fit_types += ['gfp']\n",
    "\n",
    "        # Check for combined fit\n",
    "        par_kde_combined = {}\n",
    "        if 'combined' in R[ds]:\n",
    "            hasCombined = True\n",
    "            fit_types += ['combined']\n",
    "        else:\n",
    "            hasCombined = False\n",
    "\n",
    "        # Calculate parameter distributions\n",
    "        for t in fit_types:\n",
    "            par_kde[t] = parameter_KDE(R[ds][t]['params'])\n",
    "\n",
    "        # Plot parameter distributions\n",
    "        for typeName, hasType in zip(('separate', 'single', 'combined'),\n",
    "                                     (hasSeparate, hasSingle, hasCombined)):\n",
    "            if not hasType:\n",
    "                continue\n",
    "\n",
    "            if typeName == 'single':\n",
    "                if 'rfp' in fit_types:\n",
    "                    len_pn_type = len(pn_red)\n",
    "                elif 'gfp' in fit_types:\n",
    "                    len_pn_type = len(pn_green)\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown fit types: {}\".format(fit_type))\n",
    "\n",
    "                grid = (1, len(pn_both) + len_pn_type)\n",
    "            else:\n",
    "                grid = (2, len(pn_both) + max(len(pn_red), len(pn_green)))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            gs = GridSpec(grid[0], grid[1])\n",
    "\n",
    "            if typeName == 'combined':\n",
    "                # Combined fit; define specific settings\n",
    "                pn_green_temp = pn_green\n",
    "                pn_red_temp = pn_red\n",
    "                offset_both = len(pn_both)\n",
    "                kde_label_green = 'combined'\n",
    "                kde_label_red = 'combined'\n",
    "\n",
    "                # Plot combined parameters\n",
    "                for pi, label in enumerate(pn_both):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((pi, 0), rowspan=2))\n",
    "                    data = par_kde['combined'][label]\n",
    "                    clr_face = '#0000ff55'\n",
    "                    #clr_edge = '#000099ff'\n",
    "                    plot_kde(ax, data, label, clr_face)\n",
    "            else:\n",
    "                # Separate fit; define specific settings\n",
    "                pn_green_temp = pn_both + pn_green\n",
    "                pn_red_temp = pn_both + pn_red\n",
    "                offset_both = 0\n",
    "                kde_label_green = 'gfp'\n",
    "                kde_label_red = 'rfp'\n",
    "\n",
    "            # Plot green parameters\n",
    "            if (typeName != 'single') or 'gfp' in fit_types:\n",
    "                for pi, par_label in enumerate(pn_green_temp):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((0, pi+offset_both)))\n",
    "                    data = par_kde[kde_label_green][par_label]\n",
    "                    clr_face = '#00ff0055'\n",
    "                    #clr_edge = '#009900ff'\n",
    "                    plot_kde(ax, data, par_label, clr_face)\n",
    "\n",
    "            # Plot red parameters\n",
    "            if (typeName != 'single') or 'rfp' in fit_types:\n",
    "                i_row = 0 if typeName == 'single' else 1\n",
    "                for pi, par_label in enumerate(pn_red_temp):\n",
    "                    ax = plt.subplot(gs.new_subplotspec((i_row, pi+offset_both)))\n",
    "                    data = par_kde[kde_label_red][par_label]\n",
    "                    clr_face = '#ff000055'\n",
    "                    #clr_edge = '#990000ff'\n",
    "                    plot_kde(ax, data, par_label, clr_face)\n",
    "\n",
    "            # Show and close figure\n",
    "            fig.suptitle(getDataLabel(ds) + \" (\" + typeName + \" fit)\")\n",
    "            fig.tight_layout(pad=0, rect=(0, 0, 1, .93))\n",
    "            pdf.savefig(fig, bbox_inches='tight')\n",
    "            plt.show(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot parameter correlations\n",
    "\n",
    "# Get parameters to be correlated\n",
    "par_cor = (('tr', 'tg'), ('m_ktl', 'm_ktl'), ('kmr', 'kmg'),\n",
    "           ('betr', 'betg'), ('deltr', 'deltg'), ('offr', 'offg'))\n",
    "\n",
    "for i, r in enumerate(R):\n",
    "    with PdfPages(os.path.join(getOutpath(), '{:s}_parameter_correlations.pdf'.format(getTimeStamp()))) as pdf:\n",
    "        for pr, pg in par_cor:\n",
    "            # Get parameter values\n",
    "            valr = r['red']['params'].loc[:,pr].values\n",
    "            valg = r['green']['params'].loc[:,pg].values\n",
    "\n",
    "            # Sort out outliers\n",
    "            idx = np.ones(np.size(valr), dtype=np.bool_)\n",
    "            isr = valr.argsort()[-2:]\n",
    "            isg = valg.argsort()[-2:]\n",
    "\n",
    "            if valr[isr[0]] < 0.9 * valr[isr[1]]:\n",
    "                idx[isr[1]] = False\n",
    "            if valg[isg[0]] < 0.9 * valg[isg[1]]:\n",
    "                idx[isg[1]] = False\n",
    "\n",
    "            # Plot\n",
    "            fig = plt.figure(figsize=(4.5,4))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_yscale('log')\n",
    "            ax.plot(valr[idx], valg[idx], '.')\n",
    "\n",
    "            ax.set_autoscale_on(False)\n",
    "            lmt = np.array([ax.get_xlim(), ax.get_ylim()])\n",
    "            diag = (lmt[:,0].max(), lmt[:,1].min())\n",
    "            ax.plot(diag, diag, '-k')\n",
    "\n",
    "            ax.set_xlabel(pr, color='r')\n",
    "            ax.set_ylabel(pg, color='g')\n",
    "            ax.set_title(\"{}\\nCorrelation {} – {}\".format(getDataLabel(i), pr, pg))\n",
    "\n",
    "            fig.tight_layout(pad=0)\n",
    "            plt.show(fig)\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the parameter distributions for the datasets\n",
    "ds_keys = list(R.keys())\n",
    "ds_keys.sort()\n",
    "params = R[ds_keys[0]]['combined']['params'].columns\n",
    "grid = (len(params), len(ds_keys))\n",
    "i_col = 0\n",
    "\n",
    "pdffile = os.path.join(getOutpath(), '{:s}_parameters.pdf'.format(getTimeStamp()))\n",
    "with PdfPages(pdffile) as pdf:\n",
    "    fig = plt.figure()\n",
    "    fig.set_figheight(grid[0] * .8 * fig.get_figheight())\n",
    "    fig.set_figwidth(grid[1] * .8 * fig.get_figwidth())\n",
    "\n",
    "    for ds in ds_keys:\n",
    "        i_row = 0\n",
    "        for p in params:\n",
    "            ax = plt.subplot2grid(grid, (i_row, i_col))\n",
    "            ax.hist(R[ds]['combined']['params'][p], bins=100)\n",
    "            if i_row == grid[0] - 1:\n",
    "                ax.set_xlabel('Value [a.u.]')\n",
    "            if i_col == 0:\n",
    "                ax.set_ylabel('Occurrences [#]')\n",
    "            ax.set_title('{:s}: {:s}'.format(ds, p))\n",
    "            i_row += 1\n",
    "        i_col += 1\n",
    "\n",
    "    pdf.savefig(fig)\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot onset time correlations\n",
    "pdffile = os.path.join(getOutpath(), '{:s}_onset_correlations.pdf'.format(getTimeStamp()))\n",
    "with PdfPages(pdffile) as pdf:\n",
    "    for k in R.keys():\n",
    "        fig = plt.figure()\n",
    "        plt.plot([0, 30], [0, 30], 'k-')\n",
    "        plt.plot(R[k]['combined']['params']['tr'], R[k]['combined']['params']['tg'], '.')\n",
    "        plt.xlabel('Onset RFP [h]')\n",
    "        plt.ylabel('Onset GFP [h]')\n",
    "        plt.title(k)\n",
    "        pdf.savefig(fig)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Degradation rate ratio\n",
    "def plotHistograms(maxH):\n",
    "    Rkeys = sorted(R.keys())\n",
    "    for ds in Rkeys:\n",
    "        #deltg = R[ds]['green']['params']['deltg']\n",
    "        #deltr = R[ds]['red']['params']['deltr']\n",
    "        deltg = R[ds]['combined']['params']['deltg']\n",
    "        deltr = R[ds]['combined']['params']['deltr']\n",
    "        quot = deltg / deltr\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.hist(quot, bins=150, range=(0, maxH))\n",
    "        plt.title(ds)\n",
    "        plt.xlabel('$\\delta_\\mathrm{green} / \\delta_\\mathrm{red}$ [a.u.]')\n",
    "        plt.ylabel('Occurrences [#]')\n",
    "        plt.show(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "wdg.interact(plotHistograms, maxH=wdg.IntSlider(\n",
    "    value=100, min=0, max=1000, step=10, description='Histogram maximum', continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit distribution to degradation rate quotient histograms\n",
    "def gamma(x, p=2, b=1, s=10):\n",
    "    return s * b**p * x**(p-1) * np.exp(-b * x) / sc.special.gamma(p)\n",
    "\n",
    "def gamma2(x, p1=1.9, p2=2.1, b1=0.9, b2=1.1, s1=10, s2=10):\n",
    "    return gamma(x, p1, b1, s1) + gamma(x, p2, b2, s2)\n",
    "\n",
    "def weibull(x, lmbd=.2, k=2, s=10):\n",
    "    return s * lmbd * k * (lmbd * x)**(k - 1) * np.exp(- (lmbd * x)**k)\n",
    "\n",
    "def weibull2(x, lmbd1=.15, lmbd2=.25, k1=1.9, k2=2.1, s1=10, s2=10):\n",
    "    return weibull(x, lmbd=lmbd1, k=k1, s=s1) + weibull(x, lmbd=lmbd2, k=k2, s=s2)\n",
    "\n",
    "# Define models\n",
    "model_gamma = lm.Model(gamma)\n",
    "model_gamma.set_param_hint(name='p', min=.01)\n",
    "model_gamma.set_param_hint(name='b', min=.01)\n",
    "model_gamma.set_param_hint(name='s', min=1)\n",
    "\n",
    "model_gamma2 = lm.Model(gamma2)\n",
    "model_gamma2.set_param_hint(name='p1', min=.01)\n",
    "model_gamma2.set_param_hint(name='p2', min=.01)\n",
    "model_gamma2.set_param_hint(name='b1', min=.01)\n",
    "model_gamma2.set_param_hint(name='b2', min=.01)\n",
    "model_gamma2.set_param_hint(name='s1', min=1)\n",
    "model_gamma2.set_param_hint(name='s2', min=1)\n",
    "\n",
    "model_weibull = lm.Model(weibull)\n",
    "model_weibull.set_param_hint(name='lmbd', min=.001)\n",
    "model_weibull.set_param_hint(name='k', min=.001, max=5)\n",
    "model_weibull.set_param_hint(name='s', min=1)\n",
    "\n",
    "model_weibull2 = lm.Model(weibull2)\n",
    "model_weibull2.set_param_hint(name='lmbd1', min=.001)\n",
    "model_weibull2.set_param_hint(name='lmbd2', min=.001)\n",
    "model_weibull2.set_param_hint(name='k1', min=.001, max=5)\n",
    "model_weibull2.set_param_hint(name='k2', min=.001, max=5)\n",
    "model_weibull2.set_param_hint(name='s1', min=1)\n",
    "model_weibull2.set_param_hint(name='s2', min=1)\n",
    "\n",
    "maxH = 40\n",
    "\n",
    "with PdfPages(os.path.join(getOutpath(), '{:s}_degradation_distribution.pdf'.format(getTimeStamp()))) as pdf:\n",
    "    for ds in sorted(R.keys()):\n",
    "        # Calculate degradation rate quotient\n",
    "        deltg = R[ds]['combined']['params']['deltg']\n",
    "        deltr = R[ds]['combined']['params']['deltr']\n",
    "        quot = deltg / deltr\n",
    "\n",
    "        # Create histogram\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(1, 2, 1)\n",
    "        hist_val, hist_edg = ax.hist(quot, bins=70, range=(0, maxH), label='Histogram')[:2]\n",
    "        hist_ctr = (hist_edg[:-1] + hist_edg[1:]) / 2\n",
    "\n",
    "        # Fit models\n",
    "        result_g = model_gamma.fit(hist_val, x=hist_ctr)\n",
    "        result_g2 = model_gamma2.fit(hist_val, x=hist_ctr)\n",
    "        result_w = model_weibull.fit(hist_val, x=hist_ctr)\n",
    "        result_w2 = model_weibull2.fit(hist_val, x=hist_ctr)\n",
    "\n",
    "        # Select models\n",
    "        #print('gamma: {}'.format(result_g.chisqr))\n",
    "        #print('gamma2: {}'.format(result_g2.chisqr))\n",
    "        #print('weibull: {}'.format(result_w.chisqr))\n",
    "        #print('weibull2: {}'.format(result_w2.chisqr))\n",
    "\n",
    "        if result_g2.chisqr < .7 * result_g.chisqr:\n",
    "            res_g = result_g2\n",
    "            name_g = 'gamma2'\n",
    "        else:\n",
    "            res_g = result_g\n",
    "            name_g = 'gamma'\n",
    "\n",
    "        if result_w2.chisqr < .7 * result_w.chisqr:\n",
    "            res_w = result_w2\n",
    "            name_w = 'weibull2'\n",
    "        else:\n",
    "            res_w = result_w\n",
    "            name_w = 'weibull'\n",
    "\n",
    "        # Plot models\n",
    "        x = np.linspace(.1, 5, 100)\n",
    "        ax.plot(hist_ctr, res_g.best_fit, '-', label=name_g, color='orange')\n",
    "        ax.plot(hist_ctr, res_w.best_fit, '-', label=name_w, color='magenta')\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('$\\delta_\\mathrm{green} / \\delta_\\mathrm{red}$ [a.u.]')\n",
    "        ax.set_ylabel('Counts [#]')\n",
    "        ax.set_title(ds)\n",
    "\n",
    "        # Print fit reports\n",
    "        rep = res_g.fit_report(show_correl=False) + '\\n' + res_w.fit_report(show_correl=False)\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax.set_axis_off()\n",
    "        ax.text(0, 1, rep, ha='left', va='top', family='monospace', size=5.5)\n",
    "\n",
    "        # Display, save and close figure\n",
    "        plt.show(fig)\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot of degradation rates\n",
    "Rkeys = sorted(R.keys())\n",
    "for ds in Rkeys:\n",
    "    deltg = R[ds]['combined']['params']['deltg']\n",
    "    deltr = R[ds]['combined']['params']['deltr']\n",
    "\n",
    "    fig = plt.figure()\n",
    "    h = plt.plot(deltg, deltr, '.')\n",
    "    plt.title(ds)\n",
    "    plt.xlabel('$\\delta_\\mathrm{green}$ [a.u.]')\n",
    "    plt.ylabel('$\\delta_\\mathrm{red}$ [a.u.]')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r2 = red_p.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
